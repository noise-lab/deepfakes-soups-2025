\section{Call For Action}
\label{sec:cfa}

%\chenhao{these feel a bit too generic, It would be useful to be more specific at least in a few of them.}

We reviewed the current governance framework around AI-generated content (Section \ref{sec:history}), discussed why this framework should be extended, and how governance needs to more broadly tackle how AI-generated content is changing the online information ecosystem (Section \ref{sec:discuss}). Below, we outline recommendations for different stakeholders to address how the surge of AI-generated content reshapes user experiences, creator rights, and community values and ultimately influences online trust and safety. Rather than replacing existing measures, our recommendations suggest how to extend them towards a more proactive, ecosystem-oriented governance of AI-generated content.

\subsection{Recommendations for Online Platforms}
\paragraph{Incentivize Creators to Responsibly Use Generative AI in Content Creation.} With the move to everyday users creating AI-generated content, we suggest that online platforms extend their efforts from primarily restricting and penalizing users for ``bad'' AI-generated content to actively encouraging creators' responsible practices in AI use. For example, completely banning creators from producing AI slop or downgrading low-effort AI-generated content could be infeasible in terms of overall costs and compromise legitimate use of AI in creativity (Section \ref{subsubsec:quality}). Instead, platforms could design incentives for AI-generated content involving meaningful human effort and creative insights. Similarly, platforms could encourage users to disclose their use of generative AI to make content authenticity and ownership clear. In particular, platforms could prioritize high-quality, well-disclosed AI-generated content in algorithmic recommendations and promotions, or offer additional rewards and program eligibility to creators who continue to comply with responsible practices in AI use.

Platforms could also better inform creators about ethical practices for creating and sharing AI-generated content to avoid unintentional harms in routine uses, such as creating realistic but potentially misleading content for a creative or entertainment purpose. %A recent study on short video creators reveals their urgent need for guidelines on using AI to create ``ethical and responsible'' content, particularly to avoid unintentional copyright violations and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. 
However, research reveals that to date, only a few platforms have offered concrete guidelines on how generative AI should be mindfully and responsibly used in content creation, such as AI-generated content quality guidelines~\cite{gao2026governance}. Therefore, we suggest that platforms develop responsible AI use guidelines as a part of the overall creator policies. In the guidelines, platforms could outline potential risks and harms at different stages of AI use in content creation with examples, and articulate best practices in creation and sharing, such as which kinds of AI use in content creation should be disclosed.


\paragraph{Empower Users through Educational Materials and Control Mechanisms} As AI-generated content in online spaces can potentially cause invisible damage to user cognition by increasing cognitive load, we recommend raising user awareness and increasing user autonomy in navigating the flood of AI-generated content to counter it (Section \ref{subsubsec:invisibledamage}). Prior efforts by platforms developing user empowerment resources center on promoting literacy around high-stakes deepfake manipulation (Section \ref{subsubsec:deepfakegovresponse}, Section \ref{subsubsec:AIGCgovresponse}). Researchers have also shown that labeling systems are usually the only mechanism by which users can navigate AI-generated content on online platforms and that affordances for filtering this content remain rare~\cite{gao2026governance}. Furthermore, AI-generated content labels could themselves deepen confusion and undermine user trust when mislabeling occurs (Section \ref{subsubsec:labelingtradeoff}).

Therefore, we suggest that platforms develop toolkits to proactively empower users when interacting with AI-generated content in the community. First, platforms could incorporate AI-generated content into their media literacy resources, which should provide concrete descriptions and examples on potential harms around AI-generated content, how to distinguish it, and guidance on engaging with creators in mindful and respectful ways when raising concerns about their use of AI tools. Second, platforms could provide more controls over AI-generated content in user feeds through personalized moderation~\cite{jhaver2023personalizing} and recommendation controls~\cite{li2025beyond}, mechanisms that many platforms already offer for controlling content topics. We recommend that platforms tailor these existing mechanisms to AI-generated content---for example, Pinterest has built a function ``See fewer AI pins'' as a separate recommendation feed control function.\footnote{https://help.pinterest.com/en/article/see-fewer-ai-pins}

%As these feed customization mechanisms usually work on content topics (e.g., violence) rather than provenance or production methods,

\subsection{Recommendations for AI Developers and Service Providers}
\paragraph{Include Information on Human Efforts to Content Provenance.}
Current efforts around the trust and safety of generative AI output largely focus on two perspectives: ensuring output harmlessness through content safety measures, and integrating provenance data or watermarks to help downstream actors detect whether the media is generated by AI (Section \ref{subsubsec:AIGCgovresponse}). Specifically, current industry-standard provenance on AI-generated content is valuable for transparency by offering the origin information of the content, but it provides little information about how the content is made beyond the tool it comes from. For example, ChatGPT uses C2PA in its generated images, but only shows that the image was created via the ChatGPT application or DALL-E through OpenAI API.\footnote{https://help.openai.com/en/articles/8912793-c2pa-in-chatgpt-images} Deciding ownership remains highly contested when evaluating human efforts and quality of AI-generated content to distinguish between fully automated, low effort AI-generated content and works where creators have substantial input in human efforts, personal skills, and creativity insights (Section~\ref{subsubsec:ownership}). Therefore, we see an opportunity for AI developers and service providers to provide signals related to human participation in fully AI-generated media to supplement judgments on these issues.

AI developers and service providers could extend existing provenance channels to include coarse-grained indicators of authorship and effort. For example, generative AI services could attach metadata on prompt length and complexity, approximate generation time, the number of regeneration or variation steps requested, or measures of post-editing within the product interface. AI developers and service providers could also take references from digital media editing tools' practices on metadata relating to the creation process. Notably, a good example is Adobe Photoshop, which embeds history logs of edits in image metadata.\footnote{https://helpx.adobe.com/si/incopy/using/including-metadata-story.html}  


\subsection{Recommendations for Regulators}
\paragraph{Update Current Regulations on AI-Generated Content Ownership and Copyright.} Although ownership of AI-generated content is inherently ambiguous, many generative AI services and online platforms still rely on existing copyright law to address ownership disputes (Section \ref{subsubsec:ownership}). Therefore, we see a need to update existing law to redefine copyright and ownership for AI-generated content. Specifically, we recommend that regulators should not only define who ultimately owns the intellectual property of AI-generated content, but also recognize and allocate contributions from different parties---creators who create AI-generated content, the AI model and company, the training data providers, and the model input material providers---across different uses of AI in content creation. Such reformulations could draw on current research on technical solutions and public perspectives on allocating AI-generated content copyright (e.g.,~\cite{10.1145/3715336.3735683, lima2025public}). This update would not only offer a clear legal framework for ownership disputes, but also help the platform better decide on appropriate monetization and promotion policies.

\paragraph{Regulating the Practice of Labeling AI-generated Content in Online Platforms.} As discussed in Section \ref{subsubsec:labelingtradeoff}, online platforms usually require the disclosure or auto-labeling of content as AI-generated or not, where trade-offs could arise from the non-binary nature of AI-generated content and mislabels. Existing regulations on AI-generated content transparency tend to focus on requiring provenance signals on generated content, whereas few rules impose responsibility on online platforms for the transparency of AI-generated content they host (Section \ref{subsubsec:AIGCgovresponse}). We therefore suggest that regulators develop minimum standards for platform labeling of AI-generated content---for instance, requiring clear and prominent labels for high-stakes, realistic synthetic media, while keeping lower-risk content optional regarding the potential trade-offs.

Furthermore, we suggest that regulators also address the transparency of labeling practices to further address the trade-off in AI-generated content labeling. Regulations such as the EU Digital Services Act\textsuperscript{\ref{fn:dsa}} already require very large online platforms to produce content moderation transparency reports and provide users with the right to appeal moderation decisions. Yet, labeling AI-generated content is not typically considered part of these obligations, perhaps because it is not considered an approach to mitigate systemic risks as typical content moderation does. We suggest that current transparency requirements on platform content moderation practices extend to AI-generated content labeling---for example, requiring accessible appeal mechanisms for users to contest false-positive labels, and report data on how AI-generated content is detected, labeled, and taken down.


%\paragraph{Build Guidelines on Governance Practices Under Emerging Challenges}
%At present, many online platforms continue to rely on existing content moderation frameworks for the governance of AI-generated content. Prior work also found a substantial variance of AI-specific policies across different platforms, as the development of platform governance on AI-generated content is still evolving~\cite {gao2026governance}. Although current governmental guidelines have addressed AI harms, generative AI safety, and deepfake manipulation, guidance on the broader trust and safety implications of the online distribution of AI-generated content remains limited~\cite{luna2024navigating}. \mc{what are these additional things that need to be addressed?}

%\mc{who should build this guidance, this was not clear from what you've written here, is this platforms? or regulators?}
%As emerging challenges cannot be fully covered by the current governance trajectory, we see an urgent need for official guidance, including concrete examples of emerging harms with the surge of AI-generated content, distinctions between high-stakes and low-stakes scenarios, and minimum governance responsibilities for both online platforms and AI service providers. In particular, the guidance could focus on emerging issues such as AI slop and offer recommendations on how to preserve community value while governing AI-generated content at scale.


\subsection{Recommendations for Researchers}
\paragraph{Study Emerging Challenges and Their Effects on Online Trust and Safety.}
Our paper highlights emerging challenges around AI-generated content in online spaces that cannot be fully captured by traditional harm categories. Therefore, we suggest that researchers in security, privacy, trust and safety systematically examine how these emerging challenges around AI-generated content play out in real online environments and how they affect online trust and safety. For instance, future work could measure the current landscape of emerging challenges, such as investigating the failure modes of labeling AI-generated content in the wild, and measuring how hate speech arises and spreads from AI skepticism in online discussions and comments. Future research could also focus on the human aspects of emerging challenges around AI-generated content. For example, future work could conduct user studies on how people perceive and navigate AI-generated content ownership disputes, AI slop, and failures of AI-generated content labeling in online spaces; and how creators experience, prevent, and recover from perceptual harms from AI skepticism by audiences or consequences from human-made content being mislabeled as AI-generated. 


\paragraph{Develop Countermeasures for Emerging Challenges}
To address emerging challenges around AI-generated content, we suggest that future research develop countermeasures through technical solutions and mechanism designs. For example, future work could focus on designing toolkits for creators to responsibly use AI in content creation, educational materials for users in navigating AI-generated harms, and adaptive feed controls on AI-generated content. We also encourage researchers to re-examine current governance tools for AI-generated content in consideration of emerging challenges. For example, although prior work has largely investigated AI-generated content label design on aspects such as label languages and position and their effectiveness on the viewer side~\cite{gamage2025labeling, epstein2023label, jung2025ai}, future research could take a step forward on how to design labels to indicate degrees or types of AI involvement in different AI-generated media and evaluate the trade-offs between content transparency and creator rights.