\section{Call For Actions}
\label{sec:cfa}
Through a review of the current governance framework on AI-generated content (Section \ref{sec:history}) and discussion on why such a framework should be extended (Section \ref{sec:discuss}), we claim that as generative AI and AI-generated content are becoming part of the foundational infrastructure of the online information ecosystem, governance cannot stop at mitigating explicit harm. 

Building on our position and discussion, we outline recommendations for online platforms, AI developers and providers, regulators, and researchers to address challenges on how the surge of AI-generated content reshapes user experience, creator rights, and community value that ultimately influence online trust and safety. Rather than replacing existing measures, our recommendations aim to extend them towards a more proactive, ecosystem-oriented governance of AI-generated content.

\subsection{Recommendations for Online Platforms}
\paragraph{Incentivize Creators to Responsibly Use Generative AI in Content Creation.} As many new challenges arise from everyday uses of generative AI rather than bad actors, we suggest that online platforms turn their stance from primarily restricting and punishing ``bad'' AI-generated content to actively encouraging creators' responsible practices. For example, completely banning creators from producing AI slop or downgrading low-effort AI-generated content could be infeasible in cost and compromise legitimate use of AI in creativity (Section \ref{subsubsec:quality}). Instead, platforms could design incentives for high-quality AI-generated content involving meaningful human effort, creative insights, and intellectual contribution. Similarly, platforms could encourage users to disclose their use of generative AI regarding content authenticity and ownership. In particular, platforms could prioritize high-quality, well-disclosed AI-generated content in algorithmic recommendations and promotions, or offer additional rewards and program eligibility to creators who continue to comply with responsible practices in AI use.

Moreover, it is also important for platforms to inform creators about responsible practices for creating and sharing AI-generated content in order to avoid unintentional harms in routine uses, such as creating potentially realistic but potentially misleading content with a creative or entertainment purpose. A recent study on short video creators reveals their urgent needs for guidelines on using AI to create ``ethical and responsible'' content, particularly to avoid unintentional copyright violation and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. However, studies reveal that to date, only a few platforms have offered concrete guidelines on how generative AI should be mindfully and responsibly used in content creation, such as AI-generated content quality guidelines~\cite{gao2026governance}. Therefore, we suggest that platforms develop responsible AI use guidelines as a part of the overall creator policies. In the guidelines, platforms could outline potential risks and harms at different stages of AI use in content creation with examples, and articulate best practices in creation and sharing, such as which kinds of AI use in content creation should be disclosed to avoid misleading the audience.


\paragraph{Empowering users through educational materials and controls} As AI-generated content in online spaces could cause invisible damage to user cognition, it is urgent to equip users with greater awareness and autonomy in navigating the flood of AI-generated content to counter it (Section \ref{subsubsec:invisibledamage}). However, as we reviewed in Section \ref{subsubsec:deepfakegovresponse} and Section \ref{subsubsec:AIGCgovresponse}, prior research developing user empowerment centers on promoting literacy of high-stakes deepfake manipulation. Moreover, such user support is also limited in the wild. Prior work reveals that in online platforms, labeling systems are usually the only tool for users to navigate AI-generated content, while other educational materials and user control on AI-generated content are rare-find~\cite{gao2026governance}. Yet, AI-generated content labels could themselves undermine user trust, especially when mislabeling occurs (Section \ref{subsubsec:labelingtradeoff}).

Therefore, we suggest that platforms develop toolkits to empower users when interacting with AI-generated content in the community. First, platforms could incorporate AI-generated content into their media literacy resources, which should provide concrete descriptions and examples of what generative AI and AI-generated content are, potential risks and harms around them, and how to navigate them in the community. Second, platforms could provide more controls over AI-generated content in user feeds through personalized moderation~\cite{jhaver2023personalizing} and recommendation controls~\cite{li2025beyond}, mechanisms that many platforms already offer. As these feed customization mechanisms usually work on content topics (e.g., violence) rather than provenance or production methods, we recommend that platforms tailor these existing mechanisms to AI-generated content, such as allowing users to choose whether to see AI-generated content and to what degree they want to be exposed to it.

\subsection{Recommendations for AI Developers and Service Providers}
\paragraph{Extend Safety Guardrail and Watermark-based Provenance to Authorship Information.}
Effort, such as prompt length on a fully AI-generated material, generation time, etc.

\subsection{Recommendations for Regulators}
\paragraph{Updating the current regulations for AI-generated content.} We regard that several regulations should be updated to address the emerging challenges under the prevalence of AI-generated content. First, many generative AI services and online platforms still rely on existing copyright law to deal with ownership disputes in AI-generated content, due to the ambiguity of ownership in AI-generated content (Section \ref{subsubsec:ownership}). Therefore, we recommend the existing law be updated to define copyright for AI-generated content, especially considering how AI models take part in the copyright, which is a new challenge of deciding copyright in the generative AI era compared to before, on tensions between real entities~\cite{samuelson2023generative}. Specifically, we recommend that not only defining the binary of who ultimately owns the intellectual property of AI-generated content, but also defining contribution among creators who create AI-generated content, the AI model and company, and the training data provider across different kinds of use of AI in content creation, which could take reference from current research on technical solutions and public perspectives (e.g.,~\cite{10.1145/3715336.3735683, lima2025public}). Such an update could not only offering clear legal reference on ownership dispute, but also help the platform better decide monetization.

Moreover, transparency in the distribution of AI-generated content also requires regulatory attention. Although many regions have laws requiring watermarks tagged on generations for generative AI services, only China has a rule to set responsibility for platforms on labeling everything (Section \ref{subsubsec:AIGCgovresponse}). In real practice, platforms also claim they auto-label AIGC as long as they detect it as such~\cite{gao2026governance}. Under this context, regulators could include transprency of AI-generated content in online platform as a part of the regulation to supplement existing rules in online platform safety and transparency like DSA. Specifically, develop official rules with more nuance on what has to be labeled and what not need to be labeled in online distribution. (such as only labeling high-stake realistic media is needed)

\paragraph{Building guidelines for best practice on AIGC governance}
Current governmental guidelines usually cover general AI harms, generative AI safety, and deepfake manipulation~\cite{luna2024navigating}. As the governance of AI-generated content online needs to be updated, there is an urgent need of official guidance, including case examples on emerging harms, what harms are high-stakes and what are low-stakes, governance framework on minimum governance efforts for both online platforms and AI service providers. Such guidance could especially focus on emerging concerns like AI slop and suggestions on how to maintain community value through govern AI-generated content. Also need official guidance on responsible production and distribution of AI-generated content, which to date is limited~\cite{}.

\subsection{Recommendations for Researchers}
1. Study more on new challenges and their effect on online trust and safety in the real world? like ownership, failures of labels, AI slop how users, especially the vulnerable population, react to new challenges?

measurement on what is happening around trust and safety (consequences)
in-situ

2. Develop countermeasure on new challenges and test their effectiveness?
such as contested ownership