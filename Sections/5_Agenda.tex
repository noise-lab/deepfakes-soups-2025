\section{Call For Actions}
\label{sec:cfa}

\chenhao{these feel a bit too generic, It would be useful to be more specific at least in a few of them.}

We have reviewed current governance frameworks around AI-generated content (Section \ref{sec:history}), discussed why these frameworks should be extended (Section \ref{sec:discuss}) and how governance needs to broaden the focus on mitigating explicit harm to addressing broader socio-technical concerns to address how AI-generated content is changing the online information ecosystem. Below, we outline recommendations for online platforms, AI developers and providers, regulators, and researchers to address challenges around how the surge of AI-generated content reshapes user experience, creator rights, and community values that ultimately influence online trust and safety. Rather than replacing existing measures, our recommendations suggest how to extend them towards a more proactive, ecosystem-oriented governance of AI-generated content.

\subsection{Recommendations for Online Platforms}
\paragraph{Incentivize Creators to Responsibly Use Generative AI in Content Creation.} As many new challenges arise from everyday uses of generative AI rather than use by bad actors, we suggest that online platforms turn their stance from primarily restricting and penalizing users for ``bad'' AI-generated content to actively encouraging creators' responsible practices in AI use. For example, completely banning creators from producing AI slop or downgrading low-effort AI-generated content could be infeasible in terms of overall costs and compromise legitimate use of AI in creativity (Section \ref{subsubsec:quality}). Instead, platforms could design incentives for AI-generated content involving meaningful human effort and creative insights. Similarly, platforms could encourage users to disclose their use of generative AI regarding content authenticity and ownership. In particular, platforms could prioritize high-quality, well-disclosed AI-generated content in algorithmic recommendations and promotions, or offer additional rewards and program eligibility to creators who continue to comply with responsible practices in AI use.

Moreover, it is also important for platforms to better inform creators about ethical practices for creating and sharing AI-generated content to avoid unintentional harms in routine uses, such as creating realistic but potentially misleading content for a creative or entertainment purpose. A recent study on short video creators reveals their urgent need for guidelines on using AI to create ``ethical and responsible'' content, particularly to avoid unintentional copyright violations and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. However, research reveals that to date, only a few platforms have offered concrete guidelines on how generative AI should be mindfully and responsibly used in content creation, such as AI-generated content quality guidelines~\cite{gao2026governance}. Therefore, we suggest that platforms develop responsible AI use guidelines as a part of the overall creator policies. In the guidelines, platforms could outline potential risks and harms at different stages of AI use in content creation with examples, and articulate best practices in creation and sharing, such as which kinds of AI use in content creation should be disclosed.


\paragraph{Empower Users through Educational Materials and Control Mechanisms} As AI-generated content in online spaces cn potentially cause invisible damage to user cognition by increasing cognitive load, we recommend equipping users with greater awareness and autonomy in navigating the flood of AI-generated content to counter it (Section \ref{subsubsec:invisibledamage}). Prior efforts by platforms developing user empowerment resources centers on promoting literacy around high-stakes deepfake manipulation (Section \ref{subsubsec:deepfakegovresponse}, Section \ref{subsubsec:AIGCgovresponse}). In addition, such user support is also scarcely provided by platforms. Instead, prior work reveals that in online platforms, labeling systems are usually the only mechanism by which users can navigate AI-generated content~\cite{gao2026governance}. Yet, AI-generated content labels could themselves deepen confusion and undermine user trust when mislabeling occurs (Section \ref{subsubsec:labelingtradeoff}).

Therefore, we suggest that platforms develop toolkits to proactively empower users when interacting with AI-generated content in the community. First, platforms could incorporate AI-generated content into their media literacy resources, which should provide concrete descriptions and examples of what AI-generated content is, potential harms around it, how to distinguish it, and guidance on engaging with creators in mindful and respectful ways when raising concerns about their use of AI tools. Second, platforms could provide more controls over AI-generated content in user feeds through personalized moderation~\cite{jhaver2023personalizing} and recommendation controls~\cite{li2025beyond}, mechanisms that many platforms already offer for controlling content topics. We recommend that platforms tailor these existing mechanisms to AI-generated content, such as allowing users to choose whether to see AI-generated content and to what degree they want to be exposed to it.

%As these feed customization mechanisms usually work on content topics (e.g., violence) rather than provenance or production methods,

\subsection{Recommendations for AI Developers and Service Providers}
\paragraph{Include Information on Human Efforts to Content Provenance.}
Current efforts around the trust and safety of generative AI output largely focus on two perspectives: ensuring output harmlessness through content safety measures, and integrating provenance data or watermarks to help downstream actors detect whether the media is generated by AI (Section \ref{subsubsec:AIGCgovresponse}). Specifically, current content provenance data is valuable for transparency by offering the origin information of the content, but it provides little information about how the content is made beyond the tool it comes from. Deciding ownership remains highly contested when evaluating human efforts and quality of AI-generated content to  distinguish between fully automated, low effort AI-generated content and works where creators have substantial input in human efforts, personal skills, and creativity insights (Section~\ref{subsubsec:ownership}). Therefore, we perceive an opportunity for AI developers and service providers to provide signals related to human participation in fully AI-generated media to supplement online platform judgment.

Thus, we suggest that AI developers and service providers consider extending existing provenance channels to include coarse-grained indicators of authorship and effort. For example, generative AI services could attach metadata on prompt length and complexity, approximate generation time, the number of regeneration or variation steps requested, or measures of post-editing within the product interface. 


\subsection{Recommendations for Regulators}
\paragraph{Update Current Regulations For AI-generated Content.} Several regulations require updates to address emerging challenges arising from the prevalence of AI-generated content. First, although ownership of AI-generated content is inherently ambiguous, many generative AI services and online platforms still rely on existing copyright law to address ownership disputes (Section \ref{subsubsec:ownership}). Therefore, we see a need to update existing law to redefine copyright and ownership for AI-generated content. Specifically, we recommend that regulators should not only define who ultimately owns the intellectual property of AI-generated content, but also recognize and allocate contributions from different parties---creators who create AI-generated content, the AI model and company, and the training data provider---across different uses of AI in content creation. Such reformulation could draw on current research on technical solutions and public perspectives on AI-generated content copyright (e.g.,~\cite{10.1145/3715336.3735683, lima2025public}). This update would not only offer a clear legal framework for ownership disputes, but also help the platform better decide on appropriate monetization and promotion policies.

Additionally, the transparency of AI-generated content in online distribution also needs regulatory attention. As discussed in Section \ref{subsubsec:labelingtradeoff}, online platforms usually fulfill transparency by requiring disclosure or auto-labeling content as AI-generated or not, where trade-offs could arise from the non-binary nature of AI-generated content and mislabels. Existing regulations on generative AI transparency tend to focus on AI service providers, such as requiring provenance signals on generated content, whereas few rules impose responsibility on platforms for the transparency of AI-generated content they host (Section \ref{subsubsec:AIGCgovresponse}). Under this context, we suggest regulators consider incorporating transparency requirements around AI-generated content on online platforms into the general platform safety and transparency legal frameworks, such as those in the EU Digital Service Act \mc{add citation and say what the EU DSA says about this}. Rather than only mandating platforms to detect and label AI-generated content, such requirements could also ask platforms to provide appeal pipelines for false positive AI-generated content labels. This requirement is already present in the EU Digital Service Act, where users have a right to appeal any moderation decision \mc{i edited the sentence so couldn't tell if the next part is already in the EU dSA or if you're suggesting it should be} and platforms have to report data on how AI-generated content is detected, labeled, and taken down.


%\paragraph{Build Guidelines on Governance Practices Under Emerging Challenges}
%At present, many online platforms continue to rely on existing content moderation frameworks for the governance of AI-generated content. Prior work also found a substantial variance of AI-specific policies across different platforms, as the development of platform governance on AI-generated content is still evolving~\cite {gao2026governance}. Although current governmental guidelines have addressed AI harms, generative AI safety, and deepfake manipulation, guidance on the broader trust and safety implications of the online distribution of AI-generated content remains limited~\cite{luna2024navigating}. \mc{what are these additional things that need to be addressed?}

%\mc{who should build this guidance, this was not clear from what you've written here, is this platforms? or regulators?}
%As emerging challenges cannot be fully covered by the current governance trajectory, we see an urgent need for official guidance, including concrete examples of emerging harms with the surge of AI-generated content, distinctions between high-stakes and low-stakes scenarios, and minimum governance responsibilities for both online platforms and AI service providers. In particular, the guidance could focus on emerging issues such as AI slop and offer recommendations on how to preserve community value while governing AI-generated content at scale.


\subsection{Recommendations for Researchers}
\paragraph{Study Emerging Challenges and Their Effects on Online Trust and Safety.}
Our paper highlights emerging challenges around AI-generated content in online spaces that cannot be fully captured by traditional harm categories. Therefore, we suggest that researchers in security, privacy, trust and safety systematically examine how these emerging challenges around AI-generated content play out in real online environments and how they affect online trust and safety. For instance, future work could measure the current landscape of emerging challenges, such as investigating the failure modes of labeling AI-generated content in the wild, and measuring how hate speech arises and spreads from AI skepticism in online discussions and comments. Future research could also focus on the human aspects of emerging challenges around AI-generated content. For example, future work could conduct user studies on how people perceive and navigate AI-generated content ownership disputes, AI slop, and failures of AI-generated content labeling in online spaces; and how creators experience, prevent, and recover from perceptual harms from AI skepticism by audiences or consequences from human-made content being mislabeled as AI-generated. 


\paragraph{Develop Countermeasures for Emerging Challenges}
Building upon our discussion on emerging challenges around AI-generated content, we suggest that future research develop countermeasures through technical solutions and mechanism designs. For example, future work could focus on designing toolkits for creators to responsibly use AI in content creation, educational materials for users in navigating AI-generated harms, and adaptive feed controls on AI-generated content. We also encourage researchers to re-examine current governance tools for AI-generated content in consideration of emerging challenges. For example, although prior work has largely investigated AI-generated content label design on aspects such as label languages and position and their effectiveness on the viewer side~\cite{gamage2025labeling, epstein2023label, jung2025ai}, future research could take a step forward on how to design labels to indicate degrees or types of AI involvement in different AI-generated media and evaluate the trade-offs between content transparency and creator rights.