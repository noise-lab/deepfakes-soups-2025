\section{Call For Actions}
\label{sec:cfa}
Through a review of the current governance framework on AI-generated content (Section \ref{sec:history}) and discussion on why such a framework should be extended (Section \ref{sec:discuss}), we claim that as generative AI and AI-generated content are becoming part of the foundational infrastructure of the online information ecosystem, governance should extent from mitigating explicit harm. Below, we outline recommendations for online platforms, AI developers and providers, regulators, and researchers to address challenges on how the surge of AI-generated content reshapes user experience, creator rights, and community value that ultimately influence online trust and safety. Rather than replacing existing measures, our recommendations aim to extend them towards a more proactive, ecosystem-oriented governance of AI-generated content.

\subsection{Recommendations for Online Platforms}
\paragraph{Incentivize Creators to Responsibly Use Generative AI in Content Creation.} As many new challenges arise from everyday uses of generative AI rather than bad actors, we suggest that online platforms turn their stance from primarily restricting and punishing ``bad'' AI-generated content to actively encouraging creators' responsible practices in AI uses. For example, completely banning creators from producing AI slop or downgrading low-effort AI-generated content could be infeasible in cost and compromise legitimate use of AI in creativity (Section \ref{subsubsec:quality}). Instead, platforms could design incentives for AI-generated content involving meaningful human efforts and creative insights. Similarly, platforms could encourage users to disclose their use of generative AI regarding content authenticity and ownership. In particular, platforms could prioritize high-quality, well-disclosed AI-generated content in algorithmic recommendations and promotions, or offer additional rewards and program eligibility to creators who continue to comply with responsible practices in AI use.

Moreover, it is also important for platforms to inform creators about ethical practices for creating and sharing AI-generated content in order to avoid unintentional harms in routine uses, such as creating potentially realistic but potentially misleading content with a creative or entertainment purpose. A recent study on short video creators reveals their urgent needs for guidelines on using AI to create ``ethical and responsible'' content, particularly to avoid unintentional copyright violation and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. However, research reveals that to date, only a few platforms have offered concrete guidelines on how generative AI should be mindfully and responsibly used in content creation, such as AI-generated content quality guidelines~\cite{gao2026governance}. Therefore, we suggest that platforms develop responsible AI use guidelines as a part of the overall creator policies. In the guidelines, platforms could outline potential risks and harms at different stages of AI use in content creation with examples, and articulate best practices in creation and sharing, such as which kinds of AI use in content creation should be disclosed.


\paragraph{Empower Users through Educational Materials and Control Mechanisms} As AI-generated content in online spaces could cause invisible damage to user cognition, it is urgent to equip users with greater awareness and autonomy in navigating the flood of AI-generated content to counter it (Section \ref{subsubsec:invisibledamage}). However, prior research developing user empowerment centers on promoting literacy of high-stakes deepfake manipulation (Section \ref{subsubsec:deepfakegovresponse}, Section \ref{subsubsec:AIGCgovresponse}). And such user support is also limited in the wild. Prior work reveals that in online platforms, labeling systems are usually the only tool for users to navigate AI-generated content~\cite{gao2026governance}. Yet, AI-generated content labels could themselves deepen confusion and undermine user trust, especially when mislabeling occurs (Section \ref{subsubsec:labelingtradeoff}).

Therefore, we suggest that platforms develop toolkits to proactively empower users when interacting with AI-generated content in the community. First, platforms could incorporate AI-generated content into their media literacy resources, which should provide concrete descriptions and examples of what AI-generated content is, potential harms around it, how to distinguish it, and guidance on engaging with creators in mindful and respectful ways when raising concerns about their use of AI tools. Second, platforms could provide more controls over AI-generated content in user feeds through personalized moderation~\cite{jhaver2023personalizing} and recommendation controls~\cite{li2025beyond}, mechanisms that many platforms already offer for controlling content topics. We recommend that platforms tailor these existing mechanisms to AI-generated content, such as allowing users to choose whether to see AI-generated content and to what degree they want to be exposed to it.

%As these feed customization mechanisms usually work on content topics (e.g., violence) rather than provenance or production methods,

\subsection{Recommendations for AI Developers and Service Providers}
\paragraph{Include Information on Human Efforts to Content Provenance.}
Current efforts around the trust and safety of generative AI output largely focus on two perspectives: ensuring output harmlessness through content safety measures, and integrating provenance or watermark to help downstream actors detect whether the media is generated by AI (Section \ref{subsubsec:AIGCgovresponse}). Specifically, current content provenance is valuable for transparency by offering the origin information of the content, while it provides little information about how the content is made beyond the tool it comes from. As human efforts and quality of AI-generated content remain highly contested to judge when deciding ownership, especially when distinguishing between fully automated, low effort AI-generated content and works where creators have substantial input in human efforts, personal skills, and creativity insights (Section~\ref{subsubsec:ownership}), we perceive an opportunity for AI developers and service providers to provide signals related to human participation in fully AI-generated media to supplement online platform judgment.

Thus, we suggest that AI developers and service providers consider extending existing provenance channels to include coarse-grained indicators of authorship and effort. For example, generative AI services could attach metadata on prompt length and complexity, approximate generation time, the number of regeneration or variation steps requested, or measures of post-editing within the product interface. 


\subsection{Recommendations for Regulators}
\paragraph{Update the current regulations for AI-generated content.} We regard several regulations as requiring updates to address emerging challenges arising from the prevalence of AI-generated content. First, although ownership of AI-generated content is inherently ambiguous, many generative AI services and online platforms still rely on existing copyright law to address ownership disputes (Section \ref{subsubsec:ownership}). Therefore, we see a need to update existing law to redefine copyright and ownership for AI-generated content. Specifically, we recommend that regulators should not only define who ultimately owns the intellectual property of AI-generated content, but also recognize and allocate contributors among different parties: creators who create AI-generated content, the AI model and company, and the training data provider, across different uses of AI in content creation. Such reformulation could draw on current research on technical solutions and public perspectives on AI-generated content copyright (e.g.,~\cite{10.1145/3715336.3735683, lima2025public}). This update could not only offer a clear legal framework on ownership disputes, but also help the platform better decide monetization and promotion as a reference.

Moreover, the transparency of AI-generated content in online distribution also needs regulatory attention. As discussed in Section \ref{subsubsec:labelingtradeoff}, online platforms usually fulfill transparency by requiring disclosure or auto-labeling content as AI-generated or not, where trade-offs could arise from the non-binary nature of AI-generated content and mislabels. Existing regulations on generative AI transparency tend to focus on AI service providers, such as requiring provenance signals on generated content, whereas few rules impose responsibility on platforms for the transparency of AI-generated content they host (Section \ref{subsubsec:AIGCgovresponse}). Under this context, we suggest regulators consider incorporating transparency requirements around AI-generated content on online platforms into the general platform safety and transparency legal frameworks, such as those in the EU Digital Service Act. Rather than only mandating platforms to detect and label AI-generated content, such requirements could also ask platforms to provide appeal pipelines for AI-generated content labels, as is required by user rights for typical content moderation in the EU Digital Service Act, and report data on how AI-generated content is detected, labeled, and taken down.


\paragraph{Build Guidelines on Governance Practices Under Emerging Challenges}
Right now, many online platforms continue to rely on existing content moderation frameworks for the governance of AI-generated content. Prior work also found a substantial variance of AI-specific policies across different platforms, as the development of platform governance on AI-generated content is still at an early stage after the wide adoption of generative AI~\cite {gao2026governance}. Although current governmental guidelines have addressed broadly AI harms, generative AI safety, and deepfake manipulation, guidance on the broader trust and safety implications of the online distribution of AI-generated content remains limited~\cite{luna2024navigating}. 

As emerging challenges cannot be fully covered by the current governance trajectory, we see an urgent need for official guidance, including concrete examples of emerging harms with the surge of AI-generated content, distinctions between high-stakes and low-stakes scenarios, and minimum governance responsibilities for both online platforms and AI service providers. In particular, the guidance could focus on emerging issues such as AI slop and offer recommendations on how to preserve community value while governing AI-generated content at scale.


\subsection{Recommendations for Researchers}
\paragraph{Study Emerging Challenges and Their Effects on Online Trust and Safety.}
Our discussion highlights emerging challenges around AI-generated content in online spaces that cannot be fully captured by traditional harm categories. Therefore, we suggest that researchers in security, privacy, trust and safety systematically examine how these emerging challenges play out in real online environments and how they affect online trust and safety. On the one hand, future works could measure the current landscape of emerging challenges, such as auditing the failure modes of labeling AI-generated content in the wild, and measuring how hate speech arises and spreads over time from AI skepticism in online discussions and comments. On the other hand, research could focus on the human aspect of emerging challenges. For example, future works could conduct user studies on how people perceive and navigate AI-generated content ownership disputes, AI slop, and failures of AI-generated content labeling in online spaces, and how creators experience, prevent, and recover from perceptual harms from AI skepticism by audiences or consequences from human-made content being mislabeled as AI-generated. 


\paragraph{Develop Countermeasures for Emerging Challenges}
Building upon our discussion on emerging challenges, we suggest that future research develop countermeasures through technical solutions and mechanism designs. For example, future works could work on designing toolkits for creators to responsibly use AI in content creation, educational materials for users in navigating AI-generated harms, and adaptive feed controls on AI-generated content. We also encourage researchers to re-examine current governance tools in consideration of emerging challenges. For example, although prior work has largely investigated AI-generated content label design on aspects such as label languages and position and their effective on the viewer side~\cite{gamage2025labeling, epstein2023label, jung2025ai}, future research could take a step forward on how to design labels to indicate degrees or types of AI involvement in different AI-generated media, regarding the trade-offs between content transparency and creator rights.