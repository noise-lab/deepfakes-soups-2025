\section{Introduction}
\label{sec:intro}
From the emergence of generative networks like generative adversarial networks (GANs) to large generative models like diffusion models and large language models (LLMs), generative AI has shown its increasing capability of generating high-quality, creative text and media. This progress has enabled a new content paradigm of \textit{AI-generated content}, whose production and distribution raise critical concerns about online trust and safety. To date, stakeholders---including regulators, AI practitioners, online platforms, and researchers---have been actively addressing those challenges, which we refer to as \textit{governance of AI-generated content} in online spaces. 

Recently, the rapid development of large generative models and generative AI products, together with the sudden surge of AI-generated content in online spaces, has fundamentally altered the scale, visibility, and role of AI in content consumption and digital communication~\cite{}. While the governance of AI-generated content can be traced back to the adoption of early AI techniques capable of producing realistic media, the recent landscape evolution of large models marks a departure from the conditions under which existing governance approaches initially took shape. In this paper, we revisit the development of AI-generated content governance and explore future paradigms that can better respond to the impacts of the recent large-scale proliferation of AI-generated content online.

The governance of AI-generated content began in the late 2010s, when the capability of AI in content generation was exploited to produce realistic images, videos, and audio that depict people behaving in ways they have not. Known as \textit{deepfakes} or \textit{synthetic media}, the abuse of such content has resulted in facilitating high-stakes online harms such as the dissemination of disinformation and non-consensual intimate media~\cite{}. Since then, regulators worldwide have been continuously developing legal frameworks to define the harms and liabilities associated with deepfake abuses. %In response to regulatory pressures and public concerns, online platforms have been establishing concrete pipelines encompassing policy formulation, detection, and enforcement to curb the production, spread, and misuse of deepfakes~\cite{}. 
More recently, the development of large generative models has scaled up the issue of AI-generated realistic media, while highlighting challenges of easily-produced harmful, biased, or hallucinated AI-generated output~\cite{}. This trend led to a new wave of law formulation focusing on transparency, accountability, and harm prevention of generative AI. In generative AI products, such regulations have informed providers to enforce content moderation on their end-users~\cite{}. Simultaneously, with the increasing volume of deepfake attacks and problematic AI-generated content, mainstream online communities have prioritized such content within their platform governance through policymaking and platform design~\cite{}. To support the regulation and policy enforcement, stakeholders have also been developing technical solutions targeting the safety and authenticity of AI-generated content, such as model alignment and content guardrails of generative AI~\cite{}, detection mechanisms on deepfake and other AI-generated content~\cite{}, and watermarks and provenance for AI-generated content~\cite{}. %One example is industry-standard content provenance integrated into media metadata (e.g., C2PA~\cite{}). While applied to tackle content authenticity and misleading content in general, such provenance tools have also been widely adopted by both mainstream generative AI services and online communities to support easy detection of AI-generated content~\cite{}.

Indeed, stakeholders have devoted substantial efforts to addressing the risks brought by AI-generated content. However, governance remains largely confined to explicit harms, where the problematic AI-generated content is treated as an attack to be moderated, and the threat model---attacker, victim, harm, and consequence---is usually clear. In generative AI products, the governance mostly focuses on maintaining the harmlessness of output and controlling abuse of the service, such as prohibiting the generation of disturbing, deceptive, privacy-violating, and explicit content~\cite{}. In online communities such as social media platforms, governance efforts primarily focus on enforcing existing community guidelines addressing harmful content, or improving content authenticity through labeling AI-generated content. Until now, only a few platforms have extended AI-generated content governance to more controversial areas like quality, ownership, and monetization~\cite{}, for which such topics are usually addressed through unofficial user-driven, bottom-up governance~\cite{}.

Here, we argue that the harm-dominant framework on AI-generated content governance, while essential, may be insufficient to address the broader negative impacts of prevalent, non-malicious AI-generated content in online spaces, which are usually diffuse, implicit, and difficult to delineate. For example, the prevalence of low-quality AI-generated content, no matter if the content is inherently harmful or not, could negatively affect user-perceived value and credibility of the information and even the whole community---a consideration recognized as ``AI slop'' recently~\cite{}. Since the widespread use of generative AI in content production, the ownership and authorship of AI and humans in content creation has long been a debatable topic in the creative industries and workspaces~\cite{}, which, in the context of online spaces, could trigger controversies when attributing credits and accountability in algorithmic promotion and monetization~\cite{}. As AI-mediated communication---where communicators adopt AI to modify, augment, or generate messages to accomplish communication goals~\cite{}---becomes the dominant mode of online interaction, AI-generated content will undoubtedly continue to flood online spaces with daily general use. Such a trend is likely to fundamentally reshape public speech, content ecology, and the ways people consume online information, while progressively blurring the boundaries between AI-generated and human-made content and reforming perceptions of content authenticity and value. 

Collectively, these emerging challenges and risks, though often not causing immediate damage, could eventually compromise user cognition, trust, and autonomy, as well as creator rights and community values if not well-addressed. Under this context, we suggest that the future governance of AI-generated content should extend its scope from incident-based, direct harm mitigation to a more comprehensive paradigm that addresses AI-generated content's broad socio-technical and epistemic impacts on the online ecosystem. We begin by clarifying the definition of AI-generated content and governance of AI-generated content in this paper (Section \ref{sec:def}). Then, we review the development of current harm-dominant governance of AI-generated content (Section \ref{sec:history}). After that, we discuss why governance of AI-generated content should go beyond a harm-dominant framework through looking at emergent challenges because of the shift of content ecology and online community dynamic in the era of generative AI (Section \ref{sec:discuss}). We finally propose an agenda to different stakeholders on improving the future governance of AI-generated content (Section \ref{}).

Through this paper, we contribute a conceptual reframing of future AI-generated content governance by moving beyond mitigating explicit harms toward addressing holistic socio-technical shifts shaping the online ecosystem, grounded in a synthesis of current governance practices, emerging challenges of surging AI-generated content and AI-mediated communication, and actionable agendas for stakeholders.


%The societal harms of online AI-generated content first drew widespread attention in the late 2010s, when the capability of generative networks to create realistic media was exploited by bad actors to produce realistic images, videos, and audio that depict people behaving in ways they have not, known as \textit{deepfakes} or \textit{synthetic media}~\cite{}. Since then, the distribution of such content has long been triggering severe and direct harms of online manipulation and mis/disinformation---ranging from political deception~\cite{} to non-consensual intimacy media~\cite{} and scam~\cite{}. With the development of large generative models and their integration into consumer-facing products (e.g., ChatGPT) starting from the 2020s, the harms of AI-generated content have expanded to a more massive scale. Unlike the high technical expertise once required to produce deepfakes, generative AI products enable the general public to effortlessly produce and share realistic content regardless of their technical background. Such large generative models, meanwhile, have been repeatedly warned to produce harmful~\cite{}, biased~\cite{}, or hallucinated~\cite{} content even without intention.

%The governance of AI-generated content initiated in the late 2010s, when the capability of generative networks to create realistic media was exploited to produce realistic images, videos, and audio that depict people behaving in ways they have not, known as \textit{deepfakes} or \textit{synthetic media}~\cite{}. Since then, regulators worldwide have been continuously developing, revising, and expanding regional legal frameworks to define the harms and liabilities associated with distributing such content (e.g., the U.S. Federal Take It Down Act dealing with non-consensual intimate media~\cite{}). In response to regulatory pressures and public concerns, most online platforms have been establishing concrete pipelines encompassing policy formulation, detection, enforcement, and cross-sector collaboration to curb the production, spread, and misuse of deepfakes~\cite{}. 

%More recently, the development of large generative models has scaled up the issue of AI-generated realistic media, while highlighting emerging challenges of harmful, biased, or hallucinated AI-generated output~\cite{}. This trend led to a new wave of law formulation and amendment focusing on transparency, accountability, and harm prevention of generative AI models and services (e.g., the EU AI Act~\cite{}). In generative AI products, such regulations have informed providers to enforce content moderation on their end-users, where they build up policies and safety mechanisms to prevent harmful output generations and restrict malicious accounts~\cite{}. Simultaneously, with the increasing volume of AI-generated content online because of large generative models, many mainstream online communities have prioritized AI-generated content within their existing content moderation pipeline that is designed to ensure user-generated content adheres to community standards~\cite{}. 

%To support the regulation and policy enforcement, AI practitioners, service providers, and researchers have also been developing solutions to ensure the safety of production, distribution, and usage of AI-generated content, such as safety guardrails of generative models~\cite{}, detection mechanisms on deepfake and other AI-generated content~\cite{}, and watermarks of AI-generated content~\cite{}. One example is industry-standard content provenance that integrates into media metadata (e.g., C2PA~\cite{}). While applied to tackle content authenticity issues online, such provenance tools have also been widely adopted by mainstream generative AI services in AI-generated content production, and by online communities to detect and label AI-generated content with ease.

%Now, online platforms have proactively integrated AI-generated content into their platform governance roadmap~\cite{}---they establish policies, principles, and mechanisms to manage the production, presentation, and distribution of such content---especially since the surge of AI-generated content online.

%While many providers incorporate copyrights as a part of content moderation focus because of legal compliance, as already been discussed by many lawyers and researchers, current copyright laws fall short in completely solving the controversies of ownership and authorship in AI-generated content~\cite{}.

%This overall governance trend on online platforms may be because the current regulatory requirements on AI-generated content are also harm-focused, and the most fundamental and urgent considerations of online safety are to protect users from explicit and direct harm, which should be addressed as a first priority. 