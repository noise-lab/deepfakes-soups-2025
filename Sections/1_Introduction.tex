\section{Introduction}
\label{sec:intro}
From the emergence of generative networks like generative adversarial networks (GANs) to large generative models like diffusion models and large language models (LLMs), generative AI has shown its increasing capability of generating high-quality, creative text and media. This progress has facilitated the production and distribution of \textit{AI-generated content}, raising critical challenges about online trust and safety. To date, stakeholders---including regulators, AI practitioners \lan{or AI service provider}, online platforms, and researchers---have been actively addressing those challenges, which we refer to as \textit{governance of AI-generated content} in online spaces. 

Recently, the rapid development of large generative models and generative AI products, together with the sudden surge of AI-generated content in online spaces, has fundamentally altered the scale, visibility, and role of AI in online content consumption~\cite{liang2025widespread,matatov2024examining,wei2024understanding,sun2025we,hua2024generative,guo2025exploring}. While the governance of AI-generated content can be traced back to the adoption of early AI techniques capable of producing realistic media~\cite{westerlund2019emergence}, the recent landscape evolution of large models marks a departure from the conditions under which existing governance approaches initially took shape. In this paper, we revisit the development of AI-generated content governance and explore future paradigms that can better respond to the impacts of the recent large-scale proliferation of AI-generated content online.

The governance of AI-generated content began in the late 2010s, when the capability of AI in content generation was exploited to produce realistic media that depict people behaving in ways they have not. Known as \textit{deepfakes} or \textit{synthetic media}, the abuse of such content has resulted in high-stakes online harms such as the dissemination of disinformation and non-consensual intimate media~\cite{westerlund2019emergence, umbach2024non, brigham2024violation, vaccari2020deepfakes, al2023impact}. 
More recently, the development of large generative models has scaled up the issue of AI-generated realistic media, while highlighting challenges of easily-produced biased, misleading, or otherwise AI-generated output and its abuse~\cite{menczer2023addressing, zhou2023synthetic, chen2023pathway, kelley2025generative, rawte2023survey}. In response, regulators worldwide have been continuously developing legal frameworks targeting deepfake abuses and generative AI safety~\cite{luna2024navigating, hacker2023regulating, geng2023comparing}. In online spaces, such regulations have informed content moderation of AI-generated content---decide whether to publish, remove, or flag content~\cite{kiesler2012regulating}. As for generative AI services, providers enforce content moderation on both user inputs and AI-generated outputs~\cite{mahomed2024auditing, gao2025cannot, riccio2024exploring}. Simultaneously, mainstream online communities have prioritized deepfake manipulation and problematic AI-generated content in user-generated content within their content moderation pipeline~\cite{batool2025between, gao2026governance}. To support the regulation and policy enforcement, stakeholders have also been developing technical solutions targeting the safety and authenticity of AI-generated content, such as model alignment and content guardrails for generative AI~\cite{mahomed2024auditing, ji2023beavertails, inan2023llama}, detection mechanisms on deepfake and other AI-generated content~\cite{rana2022deepfake, deng2025survey}, and watermarks and provenance for AI-generated content~\cite{deng2025survey, zhao2023provable}. 

Indeed, stakeholders have devoted substantial efforts to addressing the risks brought by AI-generated content. Yet, platform governance remains largely confined to explicit harms and follows an incident-driven logic: identifying and moderating AI-generated content that likely to cause direct harm, where the underlying threat model---attacker, victim, harm, and consequence---is typically clear and well-defined. In generative AI products, the usage policies primarily focus on preventing harmful generation and service abuse, such as prohibiting the generation of disturbing, deceptive, privacy-violating, and explicit content~\cite{gao2025cannot,riccio2024exploring,klyman2024acceptable}. In online communities such as social media, governance actions primarily focus on enforcing existing community guidelines on AI-generated content, or improving content authenticity through labeling AI-generated content. Relatively few platforms have extended AI-generated content governance to more controversial areas like quality, ownership, and monetization~\cite{gao2026governance}, for which such topics are usually addressed through unofficial user-driven, bottom-up governance~\cite{lloyd2025there,lloyd2025ai}. And even these policies remains largely restrictive~\cite{lloyd2025ai, gao2026governance}, such as completely banning the monetization of AI-generated content~\cite{gao2026governance}.

Here, we argue that the harm-dominant, incident-based governance, while essential, may be insufficient to address the broader negative impacts of prevalent, non-malicious \mc{how we do we define non-malicious, maybe we should stay away from malicious vs non-malicious framing} \lan{routinely used? I want to highlight that they are not from bad actors but everyday use}AI-generated content in online spaces, where harms are usually diffuse, implicit, and difficult to delineate. For example, the prevalence of low-quality AI-generated content, no matter if the content is inherently harmful or not, could negatively affect user-perceived value and credibility of the information and even the whole community---a consideration recognized as ``AI slop'' recently~\cite{Gross2025AISlopAP, mahdawi2025slop}. Since the widespread use of generative AI in content creation, the ownership and authorship of AI versus humans of AI-generated content has been a debatable topic in the creative industries and workspaces~\cite{10.1145/3706598.3713522,kyi2025governance}. In the context of online spaces, such controversies could trigger challenges when attributing credits and accountability in algorithmic promotion and monetization~\cite{cosen2024new}. As AI-mediated communication---where communicators adopt AI to modify, augment, or generate messages to accomplish communication goals~\cite{hancock2020ai}---becomes the dominant mode of online interaction, AI-generated content will undoubtedly continue to flood online spaces with daily general use. Such a trend is likely to fundamentally reshape the ways people consume online information, while progressively blurring the boundaries between AI-generated and human-made content and reforming perceptions of content authenticity and value~\cite{Gross2025AISlopAP, eissa2025influence}. 

Collectively, these emerging challenges and risks, though often not causing immediate damage, could eventually compromise user cognition and autonomy in content consumption, trust in online information, as well as creator rights and community values if not well-addressed. And these challenges are either not covered by the current concentration in governance, or cannot be addressed through an incident-based logic.\mc{this next sentence seems very clear about the goal of the paper} Under this context, we suggest that the future governance of AI-generated content should extend its scope from incident-based, direct harm mitigation to a more comprehensive paradigm that addresses AI-generated content's broad socio-technical and epistemic impacts on the online ecosystem. We begin by clarifying the definition of AI-generated content and governance of AI-generated content in this paper (Section \ref{sec:def}). Then, we review the development of current governance of AI-generated content adopted by stakeholders in regulations, policymaking, technical mechanisms, and research (Section \ref{sec:history}). After that, we discuss why governance of AI-generated content should extend from harm-dominant, incident-based framework to also look at emergent challenges brought by the surge of AI-generated content and everyday use of generative AI (Section \ref{sec:discuss}). We finally propose an agenda for regulators, AI practitioners, online platforms, and researchers stakeholders to improve the future governance of AI-generated content in online spaces (Section \ref{sec:cfa}).

Through this paper, we contribute a conceptual reframing of future AI-generated content governance by extending mitigating explicit harms to also address the holistic socio-technical shifts that AI-generated content is making on the online ecosystem.

\mc{you might need something somewhere that explains that you use the latest research to substantiate your claims so folks know this is based on literature, from the citations in the text throughout its not as clear what is from current literature. need something that says we examine current literature in SOUPS/HCI etc to back up these argeuments including studies we have conducted - make the paper feel more grounded, should look at how others have addressed this e.g. check the Shaowen Bardzell on feminism in HCI or Eli Blevis on sustainabilty in HCI papers} \lan{maybe add this to the paragraph above?}


%The societal harms of online AI-generated content first drew widespread attention in the late 2010s, when the capability of generative networks to create realistic media was exploited by bad actors to produce realistic images, videos, and audio that depict people behaving in ways they have not, known as \textit{deepfakes} or \textit{synthetic media}~\cite{}. Since then, the distribution of such content has long been triggering severe and direct harms of online manipulation and mis/disinformation---ranging from political deception~\cite{} to non-consensual intimacy media~\cite{} and scam~\cite{}. With the development of large generative models and their integration into consumer-facing products (e.g., ChatGPT) starting from the 2020s, the harms of AI-generated content have expanded to a more massive scale. Unlike the high technical expertise once required to produce deepfakes, generative AI products enable the general public to effortlessly produce and share realistic content regardless of their technical background. Such large generative models, meanwhile, have been repeatedly warned to produce harmful~\cite{}, biased~\cite{}, or hallucinated~\cite{} content even without intention.

%The governance of AI-generated content initiated in the late 2010s, when the capability of generative networks to create realistic media was exploited to produce realistic images, videos, and audio that depict people behaving in ways they have not, known as \textit{deepfakes} or \textit{synthetic media}~\cite{}. Since then, regulators worldwide have been continuously developing, revising, and expanding regional legal frameworks to define the harms and liabilities associated with distributing such content (e.g., the U.S. Federal Take It Down Act dealing with non-consensual intimate media~\cite{}). In response to regulatory pressures and public concerns, most online platforms have been establishing concrete pipelines encompassing policy formulation, detection, enforcement, and cross-sector collaboration to curb the production, spread, and misuse of deepfakes~\cite{}. 

%More recently, the development of large generative models has scaled up the issue of AI-generated realistic media, while highlighting emerging challenges of harmful, biased, or hallucinated AI-generated output~\cite{}. This trend led to a new wave of law formulation and amendment focusing on transparency, accountability, and harm prevention of generative AI models and services (e.g., the EU AI Act~\cite{}). In generative AI products, such regulations have informed providers to enforce content moderation on their end-users, where they build up policies and safety mechanisms to prevent harmful output generations and restrict malicious accounts~\cite{}. Simultaneously, with the increasing volume of AI-generated content online because of large generative models, many mainstream online communities have prioritized AI-generated content within their existing content moderation pipeline that is designed to ensure user-generated content adheres to community standards~\cite{}. 

%To support the regulation and policy enforcement, AI practitioners, service providers, and researchers have also been developing solutions to ensure the safety of production, distribution, and usage of AI-generated content, such as safety guardrails of generative models~\cite{}, detection mechanisms on deepfake and other AI-generated content~\cite{}, and watermarks of AI-generated content~\cite{}. One example is industry-standard content provenance that integrates into media metadata (e.g., C2PA~\cite{}). While applied to tackle content authenticity issues online, such provenance tools have also been widely adopted by mainstream generative AI services in AI-generated content production, and by online communities to detect and label AI-generated content with ease.

%Now, online platforms have proactively integrated AI-generated content into their platform governance roadmap~\cite{}---they establish policies, principles, and mechanisms to manage the production, presentation, and distribution of such content---especially since the surge of AI-generated content online.

%While many providers incorporate copyrights as a part of content moderation focus because of legal compliance, as already been discussed by many lawyers and researchers, current copyright laws fall short in completely solving the controversies of ownership and authorship in AI-generated content~\cite{}.

%This overall governance trend on online platforms may be because the current regulatory requirements on AI-generated content are also harm-focused, and the most fundamental and urgent considerations of online safety are to protect users from explicit and direct harm, which should be addressed as a first priority. 

