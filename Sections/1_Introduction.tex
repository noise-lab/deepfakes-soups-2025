\section{Introduction}
\label{sec:intro}

%From the emergence of generative networks like generative adversarial networks (GANs) to large generative models like diffusion models and large language models (LLMs), generative AI has shown its increasing capability of generating high-quality, creative text and media.
%This progress has facilitated the production and distribution of \textit{AI-generated content}, raising critical challenges about online trust and safety. To date, stakeholders---including regulators, AI service providers, online platforms, and researchers---have been actively addressing those challenges, which we refer to as \textit{governance of AI-generated content} in online spaces. 

The rapid development of large generative models and generative AI products has led to a sudden surge of \textit{AI-generated content} in online spaces. This surge has fundamentally altered the scale, visibility, and role of AI in online content consumption~\cite{liang2025widespread,matatov2024examining,wei2024understanding,sun2025we,hua2024generative,guo2025exploring} and raised critical challenges for the \textit{governance of AI-generated content} to maintain online trust and safety. While the governance of AI-generated content can be traced back to the adoption of early AI techniques capable of producing realistic media (i.e., \textit{deepfake} and \textit{synthetic media})~\cite{westerlund2019emergence}, the recent landscape evolution of large models marks a departure from the conditions under which existing governance approaches initially took shape. In this paper, we revisit the development of AI-generated content governance and explore future paradigms that can better respond to the impacts of the recent large-scale proliferation of AI-generated content online.

Already, stakeholders have devoted substantial efforts to addressing the risks brought by AI-generated content. Yet, current governance actions remain largely confined to explicit harms and follows an incident-based logic: identifying and moderating AI-generated content that maps into established online harm categories, under a relatively clear threat model of attacker, victim, harm, and consequence. In generative AI products, the usage policies primarily focus on preventing harmful generation and service abuse, such as prohibiting the generation of disturbing, deceptive, privacy-violating, and explicit content~\cite{gao2025cannot,riccio2024exploring,klyman2024acceptable}. In online communities such as social media, governance actions primarily focus on enforcing existing community guidelines on AI-generated content or improving content authenticity through labeling AI-generated content. Relatively few platforms have addressed unique challenges in AI-generated content like quality, ownership, and monetization via new rules~\cite{gao2026governance}, for which such topics are usually addressed through unofficial user-driven, bottom-up policymaking~\cite{lloyd2025there,lloyd2025ai}. And even these new policies remain largely one-cut restrictive~\cite{lloyd2025ai, gao2026governance}, such as completely banning the monetization of AI-generated content~\cite{gao2026governance}.

%The governance of AI-generated content began in the late 2010s, when the capability of AI in content generation was exploited to produce realistic media that depict people behaving in ways they have not. Known as \textit{deepfakes} or \textit{synthetic media}, the abuse of such content has resulted in high-stakes online harms such as the dissemination of disinformation and non-consensual intimate media~\cite{westerlund2019emergence, umbach2024non, brigham2024violation, vaccari2020deepfakes, al2023impact}. 
%More recently, the development of large generative models has scaled up the issue of AI-generated realistic media, while highlighting challenges of easily-produced biased, misleading, or otherwise AI-generated output and its abuse~\cite{menczer2023addressing, zhou2023synthetic, chen2023pathway, kelley2025generative, rawte2023survey}. In response, regulators worldwide have been continuously developing legal frameworks targeting deepfake abuses and generative AI safety~\cite{luna2024navigating, hacker2023regulating, geng2023comparing}. In online spaces, such regulations have informed content moderation of AI-generated content---decide whether to publish, remove, or flag content~\cite{kiesler2012regulating}. As for generative AI services, providers enforce content moderation on both user inputs and AI-generated outputs~\cite{mahomed2024auditing, gao2025cannot, riccio2024exploring}. Mainstream online communities have prioritized deepfake manipulation and problematic AI-generated content in user-generated content within their content moderation pipeline~\cite{batool2025between, gao2026governance}. To support the regulation and policy enforcement, stakeholders have also been developing technical solutions targeting the safety and authenticity of AI-generated content, such as model alignment and content guardrails for generative AI~\cite{mahomed2024auditing, ji2023beavertails, inan2023llama}, detection mechanisms on deepfake and other AI-generated content~\cite{rana2022deepfake, deng2025survey}, and watermarks and provenance for AI-generated content~\cite{deng2025survey, zhao2023provable}. 

Here, we argue that the current incident-based governance targeting explicit harms, while essential, may be insufficient to address the increasingly routine and large-scale use of generative AI in online spaces. As AI-generated content becomes pervasive, many consequential effects are diffuse, gradual, and difficult to delineate, through which harms can accumulate without clear incident boundaries. For example, high-volume, low-effort AI-generated posts (``AI slop'') can erode usersâ€™ perceived information value and credibility at the community level, even when individual items are not inherently harmful~\cite{Gross2025AISlopAP, mahdawi2025slop, hoffman2024first}. The widespread use of generative AI complicates ownership, authorship, and originality of creative works~\cite{10.1145/3706598.3713522,kyi2025governance}. In the context of online spaces, such controversies raise challenges for allocating credits and accountability in algorithmic promotion and monetization~\cite{cosen2024new}. As AI-mediated communication---where communicators adopt AI to modify, augment, or generate messages to accomplish communication goals~\cite{hancock2020ai}---becomes increasingly common in online interaction, AI-generated content is likely to further blur the boundaries between AI-generated and human-made content and reform perceptions of content authenticity and value online~\cite{Gross2025AISlopAP, eissa2025influence}. These emerging challenges, though often not causing immediate damage, could eventually compromise user online experiences, creator rights, and community values if not well-addressed. And these challenges are either not covered by the current governance concentration on explicit harms, or cannot be perfectly addressed through an incident-based logic as taken now. %\mc{this next sentence seems very clear about the goal of the paper} 

Under this context, we suggest that the future governance of AI-generated content should extend its scope from incident-based, explicit harm mitigation to a more comprehensive paradigm that addresses AI-generated content's broad socio-technical and epistemic impacts on the online information ecosystem. We begin by clarifying the definition of AI-generated content and governance of AI-generated content in this paper (Section \ref{sec:def}). Then, we review the development of current governance of AI-generated content adopted by stakeholders in regulations, policymaking, technical mechanisms, and research (Section \ref{sec:history}). After that, we discuss why governance of AI-generated content should extend from an incident-based framework focusing on mitigating explicit harms to also look at emergent challenges brought by the surge of AI-generated content and everyday use of generative AI (Section \ref{sec:discuss}). We finally propose an agenda for regulators, AI service providers, online platforms, and researchers to improve the future governance of AI-generated content in online spaces (Section \ref{sec:cfa}).

Our main contributions of this paper are to enhance trust and safety efforts in the SOUPS and related communities by providing an overview of what the current state of platform governance approaches are regarding AI-generated content governance and why they fall short of addressing holistic socio-technical shifts that AI-generated content is making on the online information ecosystem. In addition, we contribute concrete recommendations for researchers, policymakers, developers, and online platforms on how best to get ahead of this shift as generative AI continues to evolve.
%\mc{you might need something somewhere that explains that you use the latest research to substantiate your claims so folks know this is based on literature, from the citations in the text throughout its not as clear what is from current literature. need something that says we examine current literature in SOUPS/HCI etc to back up these argeuments including studies we have conducted - make the paper feel more grounded, should look at how others have addressed this e.g. check the Shaowen Bardzell on feminism in HCI or Eli Blevis on sustainabilty in HCI papers} \lan{maybe add this to the paragraph above?---already added a short review scope clarification at the beginning of section 3, but wondering if we should say more in intro}
 

