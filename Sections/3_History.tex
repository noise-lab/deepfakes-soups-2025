\section{The History and Present Governance of AI-Generated Content: A Harm-Dominant Trajectory}
\label{sec:history}
From the emergence of deepfakes and synthetic media to the recent proliferation of large generative models, stakeholders have continuously taken action to identify, analyze, and mitigate the harms and risks introduced by AI-generated content in online spaces. Below, we review the historical evolution and current landscape of AI-generated content governance.

Specifically, governance of AI-generated content has primarily developed along two harm-dominant trajectories. The first centers on the abuse of deepfakes and synthetic media, which has been a long-standing focus of AI-generated content governance until now (Section \ref{subsec:deepfakeabuse}). The second focuses on problematic content generation in large generative models, which have emerged as a growing concern alongside the rapid development and widespread adoption of generative AI technologies in recent years (Section \ref{subsec:genaisafety}). 


\subsection{The Long-Standing Governance of Deepfake and Synthetic Media Abuses}
\label{subsec:deepfakeabuse}
\subsubsection{Abusive Deepfake and Synthetic Media}
Deepfake and synthetic media typically refer to images, videos, or audio depicting people saying or doing things that they have never done. The first breakthrough in deepfake production was the introduction of GANs, the predominant technique at that time that enabled both the creation of entirely new realistic media and the manipulation of existing media through face-, body-, or voice-swapping~\cite{westerlund2019emergence}. While deepfake technologies have since been adopted by the entertainment and advertising industries to create attractive visual effects and dubbing, their abuse quickly overshadowed positive applications.

By the late 2010s, deepfakes began drawing widespread attention for their potential to cause real-world harm through online distribution. They are widely adopted for disinformation campaigns, including fake political figures and public policy misinformation~\cite{westerlund2019emergence, hwang2021effects,al2023impact}. Additionally, AI-generated voices and faces were increasingly used in impersonation or fake profiles~\cite{mink2022deepphish, ruffin2024does, yang2024characteristics}, and online fraud and scams~\cite{zhai2025hear}. In parallel, bad actors leveraged these tools to generate and distribute realistic porn, often featuring women or celebrities without consent (i.e., non-consensual intimate media)~\cite{westerlund2019emergence, al2023impact, umbach2024non,brigham2024violation} and child abuse~\cite{thiel2023generative}. As detection and enforcement lagged behind generation techniques, these malicious deepfake media circulated widely on online platforms~\cite{oxford2019deepfakes}. Such widespread content could cause emotional, reputational, and financial damage to both individuals featured and bystanders, eventually causing concerns about broadly public welfare and trust~\cite{hancock2021social, al2023impact, oxford2019deepfakes}.

At the early stage, the creation of deepfakes requires a high level of technical expertise. While the emergence of deepfake-centric online forums like Mr.DeepFakes has enabled discussions on tools and tutorials on making deepfakes, and provided marketplaces for monetizing and producing deepfakes at scale~\cite{han2025characterizing, timmerman2023studying}. Similar communications and exchanges were also found in other general-purpose, user-driven communities like Reddit~\cite{gamage2022deepfakes, timmerman2023studying}, and recently, platforms for general AI-generated media like CivitAI~\cite{ghosh2026marketplace}. These ecosystems contribute to the supply chain of deepfake production and distribution, making harmful deepfakes increasingly accessible to non-experts.

Recently, the development of large generative models has further lowered the barrier to deepfake production for non-experts. There is a rising amount of commercial generative AI products that are designed for abusive purposes, such as AI nudification applications for realistic pornography creation~\cite{gibson2025analyzing}. These applications enable people with no technical skills to create harmful deepfakes using a single input through a graphic user interface. Meanwhile, the open-sourced text-to-image (T2I) large model also triggers the rapid development of deepfake model variants, often distributed through easily accessible repositories and online communities about AI~\cite{hawkins2025deepfakes, wei2024exploring}. 

\subsubsection{Responses in Governance}
Since the emergence of deepfake exploits, governance efforts have maintained a long-standing focus on addressing explicit and high-stakes threats resulting from abuses---the spreading of disinformation, fraud, and non-consensual media---through strict bans, punishments, and evolving detection techniques. 

\paragraph{Strict Enforcement on Public-Visible Harmful Deepfakes and Their Distributors.} Starting in the early 2020s, regulators worldwide have been enforcing legal frameworks to regulate deepfakes~\cite{geng2023comparing}, with many targeting those severely damaging individuals' privacy, rights, and reputations~\cite{mania2024legal}. For example, China passed and enforced the ``Administrative Provisions on Deep Synthesis of Internet Information Services''\footnote{} in early 2023, which requires consent from individuals to be featured in deepfakes, and rules online platforms to be responsible to moderate harmful deepfakes on their own. In mid 2025, the US Federal government passed a ``Take It Down Act''\footnote{https://www.congress.gov/bill/119th-congress/senate-bill/146} that poses criminal penalties to non-consensual deepfake distributors and requires online platforms to take down such content reported within 48 hours, resulting in the shutdown of Mr.Deepfakes due to the loss of deepfake providers~\cite{cuevas2026deepfake, CBS_MrDeepfakes_Shutdown_2025}.

Deepfakes naturally fall within the scope of online platform moderation policies, which often involve the prohibition of toxic and misleading content~\cite{schaffner2024community,singhal2023sok}, all directly relevant to the abusive consequences of deepfakes. Furthermore, the pressure of complying with legal requirements---as mentioned above---has prompted platforms to adopt stricter enforcement practices on categories like non-consensual deepfakes~\cite{batool2025between}. For example, Pornhub, the largest pornographic video hosting site, claims a strict prohibition on any format of non-consensual intimacy media, including deepfakes.\footnote{https://help.pornhub.com/hc/en-us/articles/4419871787027-Non-Consensual-Content-Policy} Notably, since the early 2020s, the spread of political and health-related disinformation has positioned deepfakes as a central target in misinformation governance. At that time, several mainstream platforms such as Facebook defined deepfake and synthetic media as ``manipulative'', initiating the adoption of fact-checking and labeling to countermeasure them.\footnote{https://about.fb.com/news/2020/01/enforcing-against-manipulated-media}

\lan{add a paragragh on research on how deepfake moderation fails in practice}

\paragraph{Deepfake Detection and User Awareness.} To support regulation and policy enforcement, the technical and research communities have devoted significant attention to deepfake detection---the foundation of identifying harmful content and tracing bad actors. A wide range of automatic detection techniques has been developed since the rise of deepfakes, spanning deep-learning-based to likelihood-based methods~\cite{rana2022deepfake, westerlund2019emergence, al2023impact}. Complementing these technical detections, researchers have also explored the human perception and efforts in deepfake detection. Prior works have studied how users perceive, navigate, and detect deepfake attacks in online platforms~\cite{tahir2021seeing, umbach2024non}. Additionally, a variety of tools and educational interventions have been proposed to help users better identify and resist deepfake manipulation~\cite{hwang2021effects, zhai2025hear}.


\subsection{The Emergent Governance of Content Generation within Large Generative Models}
\label{subsec:genaisafety}
\subsubsection{Problematic AI-Generated Content in the Era of Large Models} 
The developments of large generative models like LLMs and diffusion models have drastically expanded the scope of AI-generated content creation. The launch of ChatGPT in late 2022 marked a turning point, indicating the beginning of a wave of commercial generative AI products designed for general users. Since then, the rapid development of foundation models, open-source ecosystems, and real-world products has lowered the barrier to producing AI-generated text, image, audio, and video content. And this shift has soon broadened both the scope and prevalence of harms associated with AI-generated content beyond deepfakes.

Content safety has long been a central concern for large generative models, which were warned to produce harmful outputs that pose direct risks to end-users, or worse, the broader public when exploited by bad actors~\cite{}. Specifically, general large models can be vulnerable to the fast-developing adversarial attacks that trigger the system to output prohibited content (e.g., jailbreaking LLMs~\cite{}). Even without malicious intent, these models can still generate hallucinated, biased, or otherwise harmful content~\cite{rawte2023survey}. 

In real-world products, the combination of multiple types of large generative models enables the production of a wide variety of problematic content. Realistic media---already a challenge in the early deepfake era---is now even easier to produce due to the proliferation of multimedia generation tools. They not only accelerate the production of deepfakes featuring humans~\cite{}, but also broaden the scope of realistic AI-generated media to include non-human imagery and synthetic scenes, which can also be abused for deceptive purposes~\cite{}. Importantly, unlike human deepfakes, non-human realistic AI-generated content is often not inherently illegal, making it harder to govern via existing policies. In addition to multimedia, LLM-driven tools could generate human-like language that can be used to produce hate speech~\cite{}, misinformation~\cite{}, scam messages~\cite{}, or drive malicious bots on social networks~\cite{}. Notably, LLM-generated problematic texts are found to be more persuasive for readers~\cite{} and harder to detect~\cite{} than earlier forms of machine-generated or human-written texts, due to their linguistic fluency and contextual adaptability.

With the increasing adoption of generative AI products, the production and dissemination of problematic AI-generated content are no longer limited to a small group of technically skilled bad actors. Today, problematic AI-generated content can be created and distributed at unprecedented speed, with a growing number of malicious users participating. In fact, online trust and safety harms associated with AI-generated content are already seen in widely adopted commercial products and have led to massive societal impacts. For instance, in early 2026, X's AI chatbot Grok was found to generate a large volume of explicit content. Much of this content was then shared across the X community by its creators, causing significant concerns to the victim and regulators~\cite{}.

\subsubsection{Responses in Governance}
\label{subsubsec:AIGCgovresponse}
As large generative models and generative AI products continue to advance, governance efforts have focused on both the production and distribution of AI-generated content. Stakeholders have prioritized content safety for generative AI to ensure the harmlessness of content generation. Online communities, meanwhile, have begun to (re)define rules for AI-generated content in user-generated content, mainly focusing on the existing rule compliance and potential threats of misleading and inauthentic content.

\paragraph{Generative AI Model and Product Safeguards.} The rise of AI has led to global regulatory efforts around its safety, transparency, and accountability~\cite{}, including some targeting harmful generation. For instance, both the EU AI Act\footnote{https://artificialintelligenceact.eu/} and China's Generative AI Measures\footnote{https://www.chinalawtranslate.com/en/ai-labeling/} include rules that strictly prohibit AI services designed to produce harmful or toxic content~\cite{}. Regardless, legislation relevant to content moderation on platform hosting user-generated content, such as the EU Digital Service Act,\footnote{https://digital-strategy.ec.europa.eu/en/policies/digital-services-act} is usually not applicable to generative AI services, leaving content safety principles in general-purpose generative AI relatively under-defined~\cite{hacker2023regulating}.

In practice, content safety is enforced at both model-level and product-level to address apparently malicious requests and harmful outputs. At the model level, researchers and AI practitioners have put extensive efforts into developing effective content safety mechanisms, including model alignment through post-training~\cite{ji2023beavertails}, external safety guardrails~\cite{inan2023llama, mahomed2024auditing}, and red teaming frameworks to anticipate and mitigate harmful inputs and outputs~\cite{}. These measures are often part of the internal development pipelines of major large foundation models, and serve as a first layer of defense. At the product level, many service providers deploy product-specific content moderation pipelines for their product. Similar to that of user-generated content in online communities, content moderation in generative AI products encompasses the prohibition of a wide range of inputs and outputs, automatic and human efforts to detect or prevent harmful content, and enforcement such as request denials, content removals, and user bans~\cite{}.

\paragraph{Tackling Problematic AI-Generated Content Distribution in Online Communities.}
Compared to the rising development of laws on harmful deepfakes and AI safety, there remains a lack of legal frameworks on how AI-generated content, in general, should be managed in online platforms. Until now, China is the only country that has enacted a regulation on the distribution of AI-generated content online, requiring online platforms to detect and label any public AI-generated content hosted by them, called Measures for Labeling AI-Generated Content.\footnote{https://www.chinalawtranslate.com/en/ai-labeling/} %More broadly, unlike the wealth guideline on responsible development and use of AI developed by global government affiliations, the official guideline of responsible distribution of AI-generated content is also rare~\cite{}.

As a result, for platforms hosting user-generated content, the governance of AI-generated content beyond harmful deepfakes is usually not motivated by legal compliance but driven by platforms themselves. To date, online platforms have taken two major approaches to prevent the impact of potentially problematic AI-generated content: enforcing existing content moderation pipelines and by disclosing and labeling AI-generated content~\cite{}. Similar to the moderation of harmful deepfakes, the existing content moderation policies and mechanisms are naturally applicable to AI-generated content that causes harm to the community~\cite{}. Nonetheless, many mainstream platforms have now emphasized AI-generated content as an emerging moderation priority, highlighting that such content is subject to the same standards and accountability mechanisms as traditional user-generated content~\cite{}. Meanwhile, disclosing and labeling AI-generated content has recently emerged as a governance strategy adapted from earlier approaches to flagging and labeling manipulative media~\cite{}. The research community also shows evidence on how AI-generated content labels in the online community could affect user engagement, trust, and awareness of misleading content~\cite{}, indicating its potential as an effective governance approach.


\paragraph{AI-Generated Content Detection in the Era of Large Models.}
Similar to the efforts on deepfake detections, researchers and AI practitioners have developed and kept updating technical methods to detect AI-generated text~\cite{} and media~\cite{} since the rise of large generative models. However, the current detection of AI-generated content has been blamed for low accuracy and bias~\cite{}. Beyond automatic detection, prior works have also found humans, even experts, to be unreliable in identifying a variety of forms of AI-generated content~\cite{boutadjine2025human, ha2024organic}, especially given its fast-evolving pattern.

As such, stakeholders have been exploring alternative ways to effectively identify AI-generated content to better support governance. One remarkable method is to integrate invisible watermark, credential, and provenance information during the content generation process~\cite{}. While these approaches were also proposed in earlier deepfake detections, they have now been widely used in real practice. Watermark and provenance-based approaches have been regarded by legislation as a required governance tool to enhance the transparency of generative AI services. California State Law\footnote{}, the EU AI Act, and China's Generative AI Measures all require AI service provider attach machine-readable signals to generated content. Industry practitioners have started the wide adoption of industry-standard provenance (e.g., C2PA~\cite{}). While applied to tackle content authenticity and misleading content issues in general, such provenance tools have been integrated in both mainstream generative AI services and online communities to support cross-platform collaborative, easy detection of AI-generated content. This method particularly supports the auto-labeling of AI-generated content in online communities~\cite{}.
