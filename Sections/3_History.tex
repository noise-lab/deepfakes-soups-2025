\section{The History and Present Governance of AI-Generated Content}
\label{sec:history}
From the emergence of deepfakes and synthetic media to the recent proliferation of large generative models, different stakeholders---such as regulators, AI services providers, online platforms, and researchers---have continuously taken actions to identify, analyze, and mitigate the harms and risks introduced by AI-generated content in online spaces. Below, we review the historical evolution and current landscape of AI-generated content governance.

Our review is not intended as a systematic literature survey. Instead, it is a problem-driven synthesis aimed at clarifying the dominant governance trajectory of AI-generated content. We organize evidence around how AI-generated content is framed as a problem in online trust and safety, and how governance is enacted through regulations and platform policies, as well as supported or supplemented by other efforts such as technical mechanisms and research works. We draw from four types of sources: governmental regulations, platform policies, news-reported incidents, and prior research in AI, Human-Computer Interaction (HCI), Computer Security, Social Science, and related communities.

Through our review, we find governance of AI-generated content has primarily developed around two problem streams. The first centers on deepfakes and synthetic media abuses, which have been a long-standing focus of AI-generated content governance until now (Section \ref{subsec:deepfakeabuse}). The second focuses on problematic content generation of large generative models, which has emerged as a growing concern alongside the rapid development and widespread adoption of generative AI technologies in recent years (Section \ref{subsec:genaisafety}). 


\subsection{The Long-Standing Governance of Deepfakes and Synthetic Media Abuses}
\label{subsec:deepfakeabuse}
\subsubsection{Abusive Deepfake and Synthetic Media}
Deepfakes and synthetic media typically refer to hyper-realistic images, videos, or audio made by deep learning technologies depicting people saying or doing things that they have never done~\cite{westerlund2019emergence, gamage2022deepfakes}. The first breakthrough in deepfake production was the introduction of generative adversarial networks (GANs), the predominant technique at that time that enabled both the creation of entirely new realistic media and the manipulation of existing media through face-, body-, or voice-swapping~\cite{westerlund2019emergence}. While deepfake technologies have since been adopted by the entertainment and advertising industries to create attractive visual effects and dubbing, their abuse quickly overshadowed positive applications.

By the late 2010s, deepfakes began drawing widespread attention for their potential to cause real-world harm through online distribution. This kind of content was at first widely adopted for disinformation campaigns, including for creating fake depictions of political figures and producing fake news ~\cite{westerlund2019emergence, hwang2021effects,al2023impact}. Additionally, AI-generated voices and faces were increasingly used in impersonated or fake profiles~\cite{mink2022deepphish, ruffin2024does, yang2024characteristics}, and online fraud and scams~\cite{zhai2025hear, ferrara2019history}. In parallel, bad actors leveraged these tools to generate and distribute realistic porn, often featuring women or celebrities without consent (i.e., non-consensual intimate media)~\cite{westerlund2019emergence, al2023impact, umbach2024non,brigham2024violation} and child abuse~\cite{thiel2023generative}. As detection and enforcement lagged behind generation techniques, these malicious deepfake media circulated widely on online platforms~\cite{oxford2019deepfakes}. Such widespread content causes emotional, reputational, and financial damage to both individuals featured and bystanders, so it raises concerns broadly about maintaining public welfare and trust when content viewers can no longer trust content at face value for authenticity~\cite{hancock2021social, al2023impact, oxford2019deepfakes}.

In the early stages, the creation of deepfakes required a high level of technical expertise. Later, the emergence of deepfake-centric online forums like Mr.DeepFakes enabled discussions on tools and tutorials for making deepfakes, and provided marketplaces for monetizing and producing deepfakes at scale~\cite{han2025characterizing, timmerman2023studying}. Similar communications and exchanges to lower the barrier to creating deepfakes were also found in other general-purpose, user-driven communities like Reddit~\cite{gamage2022deepfakes, timmerman2023studying}, and recently, platforms for general AI-generated media like CivitAI~\cite{ghosh2026marketplace}. These ecosystems contribute to the supply chain of deepfake production and distribution, making harmful deepfakes increasingly accessible to non-experts.

Recently, the development of large generative models has further lowered the barrier to deepfake production for non-experts. There is an increasing number of commercial generative AI products designed for abusive purposes, such as AI nudification applications for realistic pornography creation~\cite{gibson2025analyzing}. These applications enable people with no technical skills to create harmful deepfakes using a single input in a graphical user interface. The open-sourced text-to-image (T2I) large model also triggers the rapid development of deepfake model variants, often distributed through easily accessible repositories and online communities about AI~\cite{hawkins2025deepfakes, wei2024exploring}. 

\subsubsection{Responses in Governance}
\label{subsubsec:deepfakegovresponse}
Since the emergence of deepfakes, governance efforts have maintained a long-standing focus on addressing explicit and high-stakes threats resulting from abuses---the spreading of disinformation, fraud, and non-consensual media---through strict bans, punishments, and evolving detection techniques. 

\paragraph{Strict Enforcement on Public-Visible Harmful Deepfakes and Their Distributors.} Starting in the early 2020s, regulators worldwide have been enforcing legal frameworks to regulate deepfake distribution, with many targeting those that severely damage individuals' privacy, rights, and reputations~\cite{mania2024legal, geng2023comparing}. For example, China passed and enforced the ``Administrative Provisions on Deep Synthesis of Internet Information Services''\footnote{https://www.chinalawtranslate.com/en/deep-synthesis/} in early 2023, which requires consent from individuals to be featured in deepfakes, and rules online platforms to be responsible for detecting and removing harmful deepfakes on their own. In mid 2025, the US Federal government passed a ``Take It Down Act''\footnote{https://www.congress.gov/bill/119th-congress/senate-bill/146} that poses criminal penalties to non-consensual deepfake distributors and requires online platforms to take down such content reported within 48 hours. Such enforcement has led to significant backlashes, such as the shutdown of Mr.Deepfakes~\cite{cuevas2026deepfake, CBS_MrDeepfakes_Shutdown_2025}.

Deepfakes naturally fall within the scope of online platform content moderation--- platforms decide whether to publish, remove, or flag user-generated content~\cite{kiesler2012regulating}. Content moderation policies often involve the prohibition of toxic and misleading content~\cite{schaffner2024community,singhal2023sok}, all directly relevant to the abusive consequences of deepfakes. Furthermore, the pressure of complying with legal requirements---as mentioned above---has prompted platforms to adopt stricter enforcement practices on categories like non-consensual deepfakes~\cite{batool2025between}. For example, Pornhub, the largest pornographic video hosting site, claims a strict prohibition on any format of non-consensual intimacy media, including deepfakes.\footnote{https://help.pornhub.com/hc/en-us/articles/4419871787027-Non-Consensual-Content-Policy} Notably, since the early 2020s, the spread of political and health-related disinformation has positioned deepfakes as a central target in misinformation governance. At that time, several mainstream platforms such as Facebook defined deepfake and synthetic media as ``manipulative'', initiating the adoption of fact-checking and labeling to countermeasure them.\footnote{https://about.fb.com/news/2020/01/enforcing-against-manipulated-media}

As deepfake distribution is fundamentally governed through the existing content moderation mechanisms on platforms, the research community has also investigated the effectiveness and failures of current moderation when enforcing on deepfakes, such as the effectiveness of user reports~\cite{qiwei2024deepfakes} and biases of human moderator decision-making~\cite{mink2024s}. Regarding the legislation demanding online platforms' responsibilities for moderating deepfakes, a growing body of work also examines the legal compliance in platform governance actions~\cite{batool2025between, cuevas2026deepfake}.

\paragraph{Deepfake Detection and User Awareness.} To support regulation and policy enforcement, the technical and research communities have devoted significant attention to deepfake detection---the foundation of identifying harms and tracing bad actors. A wide range of automatic detection techniques have been developed since the rise of deepfakes, spanning deep-learning-based to likelihood-based methods~\cite{rana2022deepfake, westerlund2019emergence, al2023impact}. Complementing these technical detections, researchers have also explored the human perception and efforts in deepfake detection. Prior works have studied how users perceive, navigate, and detect online deepfake manipulations in the wild~\cite{tahir2021seeing, umbach2024non, mink2022deepphish}. Additionally, a variety of tools and educational interventions have been proposed to help users better identify and resist deepfake manipulation~\cite{hwang2021effects, zhai2025hear}.

%\mc{need a way to transition out of deepfakes to this new section}

\subsection{The Emergent Governance of Content Generation within Large Generative Models}
\label{subsec:genaisafety}
\subsubsection{Problematic AI-Generated Content in the Era of Large Models} 
The developments of large generative models like large language models (LLMs) and diffusion models have drastically expanded the scope of AI-generated content creation. The launch of ChatGPT in late 2022 marked a turning point, indicating the beginning of a wave of commercial generative AI products designed for general users. Since then, the rapid development of foundation models, open-source ecosystems, and real-world products has enabled the general public to produce AI-generated text, image, audio, and video content. This shift has also broadened both the scope and prevalence of harms associated with AI-generated content beyond deepfakes.

Content safety has long been a central concern for large generative models, which were warned to easily produce harmful outputs that pose direct risks to end-users, or worse, the broader public when exploited by bad actors~\cite{gao2025cannot}. Specifically, general large models can be vulnerable to fast-developing adversarial attacks that trigger these systems to output prohibited content (e.g., jailbreaking LLMs~\cite{yi2024jailbreak}). Even without malicious intent, these models can still generate hallucinated, biased, or otherwise harmful content~\cite{rawte2023survey, wan2023kelly}. 

In real-world products, the combination of multiple types of large generative models enables the production of a wide variety of problematic content. Realistic media---already a challenge in the early deepfake era---is now even easier to produce due to the proliferation of multimedia generation tools. These tools not only accelerate the production of malicious deepfakes featuring humans~\cite{gibson2025analyzing}, but also broaden the scope of realistic AI-generated media to include non-human imagery and synthetic scenes, which can cause content authenticity concerns in distribution and be abused for deceptive purposes~\cite{peng2025crafting, lu2023seeing}. For example, in mid 2025, an AI-generated short video of bunnies on a trampoline went viral, leading many viewers to question whether the scene was authentic~\cite{media2025aibunnies}. Such non-human realistic media is often not inherently illegal, making it harder to govern via existing policies. In addition to multimedia, the advance of LLMs enables the generation of human-like language that can be used to produce hate speech~\cite{shen2025hatebench}, misinformation~\cite{zhou2023synthetic}, scam messages~\cite{roy2024chatbots}, or drive human-like malicious bots on social networks~\cite{yang2023anatomy}. Notably, researchers have pointed out that LLM-generated problematic texts are more persuasive for readers~\cite{bai2025llm, salvi2025conversational} and harder to identify~\cite{zhou2023synthetic} than earlier forms of machine-generated texts (e.g., text generated from small language models) or human-written texts.

A study with online trust and safety experts suggests their observations and worries on how generative AI nowadays lowers the technical barrier for attackers, scales the attack speed, and generates polished content for attackers on domains such as child safety abuse and violent extremism~\cite{kelley2025generative}. In fact, online trust and safety harms associated with AI-generated content are already seen in widely adopted commercial products and have led to massive societal impacts. For instance, in early 2026, X's AI chatbot Grok was found to generate a large volume of explicit content. Much of this content was then shared across the X community by its creators, causing significant concerns to the victim and regulators~\cite{nytimes2026grok}.

\subsubsection{Responses in Governance}
\label{subsubsec:AIGCgovresponse}
As large generative models and generative AI products continue to advance, governance efforts have focused on both the production and distribution of AI-generated content. Regulators and AI developers have prioritized content safety for generative AI tools to ensure the harmlessness of content generation. Online communities, meanwhile, have begun to (re)define rules for AI-generated content in user-generated content, mainly focusing on enforcing existing content moderation policies and mitigating potential threats of inauthentic content through labeling.

\paragraph{Generative AI Model and Product Safeguards.} The rise of AI has led to global regulatory efforts around its safety, transparency, and accountability~\cite{luna2024navigating}, including some targeting harmful generation. For instance, both the EU AI Act\footnote{\label{fn:euaiact}https://artificialintelligenceact.eu/} and China's Generative AI Measures\footnote{\label{fn:genaichina}https://www.chinalawtranslate.com/en/ai-labeling/} include rules that strictly prohibit AI services designed to produce harmful or toxic content. Regardless, legislation relevant to content moderation on platform hosting user-generated content, such as the EU Digital Services Act,\footnote{\label{fn:dsa}https://digital-strategy.ec.europa.eu/en/policies/digital-services-act} is usually not applicable to generative AI services, leaving content safety principles in general-purpose generative AI relatively under-defined~\cite{hacker2023regulating}.

In practice, content safety is enforced at both model-level and product-level to address apparently malicious requests and harmful outputs. In foundation model development, researchers and AI developers have developed effective content safety mechanisms, including model alignment through post-training~\cite{ji2023beavertails}, external safety guardrails~\cite{inan2023llama}, and red teaming frameworks to anticipate and mitigate harmful inputs and outputs~\cite{feffer2024red}. These measures are often part of the internal development pipelines of major large foundation models, and serve as a first layer of defense. In generative AI products such as ChatGPT and Midjourney, many service providers deploy product-specific content moderation pipelines for their products~\cite{mahomed2024auditing, gao2025cannot, riccio2024exploring}. These content moderation policies in generative AI products prohibit a wide range of inputs and outputs, use automatic and human efforts to detect or prevent harmful content, and enforce rules through request denials, content removals, and user bans~\cite{gao2025cannot}.

\paragraph{Tackling Problematic AI-Generated Content Distribution in Online Communities.}
Compared to the rising development of laws on harmful deepfakes and AI safety, there remains a lack of legal frameworks on how AI-generated content, in general, should be managed in online platforms. Until now, China is the only country that has enacted a regulation on the distribution of AI-generated content online, requiring online platforms to detect and label any public AI-generated content hosted by them, called Measures for Labeling AI-Generated Content.\footnote{https://www.chinalawtranslate.com/en/ai-labeling/} %More broadly, unlike the wealth guideline on responsible development and use of AI developed by global government affiliations, the official guideline of responsible distribution of AI-generated content is also rare~\cite{}.
%\mc{has the EU not done this?}

As a result, for platforms hosting user-generated content, the governance of AI-generated content beyond regulating harmful deepfakes is usually not motivated by legal compliance but driven by platforms themselves. To date, online platforms have taken two major approaches to prevent the impact of potentially problematic AI-generated content: enforcing existing content moderation pipelines and by disclosing and labeling AI-generated content~\cite{gao2026governance}. Similar to the moderation of harmful deepfakes, the existing content moderation policies and mechanisms of online platforms are naturally applicable to AI-generated content that causes harm to the community~\cite{fisher2024moderating}. Nonetheless, many mainstream platforms have now emphasized AI-generated content as an emerging moderation priority, highlighting that such content is subject to the same standards and accountability mechanisms as traditional user-generated content~\cite{gao2026governance}. Meanwhile, disclosing and labeling AI-generated content has recently emerged as a governance strategy adapted from earlier approaches to flagging and labeling manipulative media~\cite{martel2024fact, martel2023misinformation}. The research community also shows evidence on how AI-generated content labels in the online community could affect user engagement, trust, and awareness of misleading content~\cite{epstein2023label, gamage2025labeling, Wittenberg2024Labeling, jung2025ai,burrus2024unmasking, wittenberg2025labeling}, indicating its potential as an effective governance approach.


\paragraph{AI-Generated Content Detection in the Era of Large Models.}
Similar to the efforts on deepfake detections, researchers and AI developers have developed and kept updating technical methods to detect AI-generated text~\cite{fraser2025detecting} and media~\cite{deng2025survey} since the rise of large generative models. However, the current detection of AI-generated content has been blamed for low accuracy and bias~\cite{boutadjine2025human, weber2023testing}. Beyond automatic detection, prior research has also explored human awareness of various forms of AI-generated content (e.g., text messages, images, and art) and found that humans, even domain experts, are unreliable in identifying it~\cite{boutadjine2025human, ha2024organic, lu2023seeing, zhou2023synthetic}, especially given its fast-evolving pattern.

As such, stakeholders have been exploring alternative ways to effectively identify AI-generated content to better support governance. One remarkable method for detecting AI-generated media is to integrate invisible watermarks, credentials, and provenance information during content generation~\cite{deng2025survey, zhao2023provable}. While these approaches were also proposed in earlier deepfake detections, they have now been widely used in practice. Watermark and provenance-based approaches have been regarded by legislation as a required governance tool to enhance the transparency of generative AI services. California AI Transparency Act,\footnote{https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill\_id=202320240SB942} the EU AI Act,\textsuperscript{\ref{fn:euaiact}} and China's Generative AI Measures\textsuperscript{\ref{fn:genaichina}} all require AI service providers to attach machine-readable signals to generated content. Industry practitioners have started the wide adoption of industry-standard provenance (e.g., C2PA~\cite{feng2023examining, C2PA2025,burrus2024unmasking}). While applied to tackle content authenticity and misleading content issues in general, such provenance tools have been integrated in both mainstream generative AI services and online communities to support cross-platform collaborative, easy detection of AI-generated content. This method particularly supports the auto-labeling of AI-generated content in online communities~\cite{gao2026governance}.

%\lan{Should I say something here on educational tool are mostly focus on deepfake media (as mentioned in 3.1.2), while limited is on general awareness of AI-generated content. Many studies have looked at improving the label design of AI-generated content disclosure though}