\section{Why Future Governance of AI-Generated Content Should Go Beyond Harm-Dominant?}
\label{sec:discuss}
As reviewed in Section \ref{sec:history}, the governance of AI-generated content has been largely coalesced around mitigating explicit harm. From deepfake manipulation to inauthentic AI-generated content, platform responses have followed an incident-driven paradigm---identifying specific harm categories, problematic content, and bad actors, then responding through content moderation enforcement.

While this harm-dominant trajectory is indeed necessary, particularly given the severe consequences of high-stakes abuses such as non-consensual intimate media and scams, it may be insufficient for addressing the broader governance needs of AI-generated content now and in the future.
The landscape of the problem has shifted: deepfakes and overt abuse of AI-generated content represent only the tip of the iceberg. The more profound and large-scale disruption lies beneath the surface---usually in the routine, non-malicious, yet pervasive integration of generative AI into everyday digital content production and communication. These uses may not fit traditional online harm categories but still pose systemic challenges to user experiences, creator authority, and community values, which fall beyond the scope of the majority harm-dominant governance framework.

Below, we first discuss how the paradigm of AI-generated content in online spaces has changed---and will continue to evolve---because of the transition of generative AI usage pattern (Section \ref{subsec:shift}). Building on this context, we then discuss how shifts in the online content ecology may give rise to contested (Section \ref{subsec:contested}), implicit and gradual (Section \ref{subsec:implicit}), and unintentional (Section \ref{}) harms that may not be well-addressed by conventional governance approaches targeting explicit harms.

\subsection{The Paradigm Shift: From Malicious-Focused to Everyday Content Production and Communication}
\label{subsec:shift}
Before the surge of large generative models and generative AI products, the production and distribution of AI-generated content in online spaces were largely limited in their scope, dominated by malicious rather than creative purposes. Although GANs have been used in creative digital arts since their development, they are mostly adopted by experts in professional settings, where controversies often emerge in real-world commercial uses rather than through widespread online distribution~\cite{}. Some online content creators at that time also adopted AI technology to create fun or satirical content, although these creations are mostly low-quality cheapfakes~\cite{}. Beyond this, most AI-generated content in online spaces at that time was deemed to be harmful deepfakes, mostly created by bad actors~\cite{}. For example, a report showed that by 2019, over 90\% of online deepfake videos were pornographic and other forms of harm~\cite{}.

Such a malicious-centric online distribution pattern beforehand was not coincidental, as the technical barrier to creating high-quality AI-generated content was high, making it inaccessible to most people. The innovation in large generative models, then, suddenly shifted generative AI technology from a high-technical, professional tool to an easy-to-use everyday application. To date, generative AI has become a common content production tool for everyone, widely used in areas like creative writing, art, and music by both professional creators and general users~\cite{}. More broadly, the generation capability of generative AI has been applied beyond facilitating creativity. With the increasing adoption of generative AI in daily life, AI-mediated communication, where the communicator leverages AI to modify, augment, and generate messages, has become the dominant pattern in digital communication. The concept of AI-mediated communication was first proposed by Hancock and colleagues in 2020, when AI techniques were most widely used to facilitate text-based communication through grammar check, auto-reply, and auto-complete a message~\cite{}. The prevalence of generative AI later expanded AI-mediated communication to AI-generated messages, human-AI co-writing, and multimodal-based communication~\cite{}.  

The paradigm shift in generative AI technology usage nowadays has also led to a significant change in online content ecology. Online platforms have witnessed a surge of volume in AI-generated content---spanning a variety of media such as text, image, and art~\cite{}. Generative AI has been widely adopted by users to construct profiles, write comments, and online reviews~\cite{}.  Online content creators also increasingly use generative AI to facilitate content creation. For example, video creators in social media are increasingly integrating generative AI in their workflow from ideation to video editing~\cite{}. Generative AI has also incentivized many new creators to produce and share their creations~\cite{}. Meanwhile, online platforms themselves also proactively integrate AI-generated content as a part of their ecosystem. Many online platforms hosting user-generated content have provided generative AI tools for users and creators, encouraging them to make and share high-quality, creative content empowered by AI~\cite{}. Platforms also use generative AI to create platform-generated content like AI search summaries, to facilitate the content consumption of users~\cite{}. 

Undoubtedly, the risks come from the malicious production of AI-generated content have also been intensified by the introduction of generative AI as an everyday technology. However, the paradigm shift of AI-generated content production and distribution---where AI components are increasingly and invisibly integrated into the whole online content ecology---has contributed to broader harms. We discuss how these harms are different from those of the malicious, inherently problematic AI-generated content in the rest of this section.

\subsection{``Harms'' are Contested}
\label{subsec:contested}
In the current harm-centric governance framework, the harms caused by problematic AI-generated content are often treated as clear-cut. 
However, as AI-generated content keeps flooding online spaces, controversies around AI-generated content itself give rise to potential harms whose boundaries are difficult to define. Issues such as ownership and content quality---already under-emphasized in existing governance~\cite{}---become even more salient in this context. Neither can these concerns be fully addressed by the harm-centric governance strategy, nor can a one-size-fits-all governance approach following a harm-centric logic be fair enough for creators and users. 

Below, we first discuss contested harms on ownership and quality of AI-generated content that arise from the nature of such content and its wide distribution. We then discuss how the current approach to labeling AI-generated content---typically for improving community transparency by tackling inauthenticity and ownership debates---can introduce trade-offs and lead harms. 


\subsubsection{The Debatable Ownership of AI-Generated Content}
Controversies around ownership and authorship of AI-generated content have become salient in online communities. Generally, unclear ownership can lead to intellectual property disputes and complicate the commercial use of a work~\cite{}. In creator economies, these debates map directly onto questions of content originality---an important factor shaping platform decisions around promotion, monetization, and creator rewards~\cite{}. As generative AI is increasingly used in every aspect of content creation, it becomes harder to determine who should be credited for a given piece of content and to what extent. The growing trend of using AI-generated content produced with minimal effort to gain influence or income further intensifies concerns about how creators' human labor should be fairly treated~\cite{}. 

Debates about ownership and authorship of AI-generated work predate the pervasion of generative AI as an everyday application~\cite{}. And now, there is a broad recognition that existing copyright law is not comprehensive enough to address AI-generated content~\cite{}, even though generative AI services and many online communities continue to rely on traditional copyright law as the primary mechanism for ownership disputes~\cite{}. Researchers have proposed several ideas for calculating credits of AI-generated content in the production phase, such as by estimating human controls in content generation~\cite{} and how different training data contribute to the output~\cite{}. However, there is still no consensus on how to define fair ownership until now.

Additionally, public perceptions of AI-generated content ownership could diverge from technical or legal framings. And even worse, perception divergences are widely regarded among different groups, such as between viewers who consume AI-generated content and creators who produce AI-generated content. Viewers tend to devalue creator credit in AI-generated work, and credit the AI model and the training data provider~\cite{}. In contrast, creators often overestimate their ownership of human-AI co-creation, and sometimes even attribute all credits to themselves rather than AI (i.e., AI ghostwriter effect)~\cite{}.

Under this context, platforms face difficulties about how to allocate algorithmic visibility, incentives, and financial rewards among creators who use AI to different degrees and for different purposes. A few platforms have responded by outright banning AI-generated content from monetization, or even from general post and sharing, citing the reason as preserving originality~\cite{}. However, these binary governance strategies are likely to fail under the circumstances where the use of generative AI in content creation is becoming ubiquitous.

\subsubsection{The Unclear Boundary and Value of Low-Quality AI-Generated Content}
Another site of contested harm lies in debates over the high volume, low quality AI-generated content made with low efforts in online spaces, often cited as ``AI slop'' recently~\cite{}. On the surface, the blame on AI slop resembles that on traditional spam: it can flood feeds and degrade users' perceived information quality~\cite{}. However, unlike classic spam, which is typically defined by malicious or opportunistic intent with overlapping categories like scam and phishing~\cite{}, low-quality AI-generated content often lacks a clearly malicious purpose. 

Moreover, low-quality AI-generated content is not uniformly valueless from the perspective of users and communities. AI-generated writings, videos, and images can be perceived as entertaining, playful, or creatively inspiring, despite being produced with minimal human effort~\cite{}. Prior work also has shown that users appreciate creators who produce low-effort AI-generated content at the same level as, or even more than, human creators, as long as they find the creation amusing or attractive~\cite{}. The phenomenon of AI slop, as already argued by some researchers, opens up an opportunity to supply the high cultural and economic demand of content consumption with its own aesthetic value~\cite{}.

Under these conditions, it becomes difficult to draw a clear line between ``harmful'' low-quality AI content that undermines user experience and contributive one. For platforms, adopting a strict governance stance against AI slop suppresses legitimate forms of participation and creativity, while a more permissive approach may exacerbate concerns about information ecosystem pollution. This tension makes the governance of low-quality AI-generated content inherently contested rather than a straightforward harm.


\subsubsection{The Trade-Offs of Labeling AI-Generated Content}
As reviewed in Section \ref{subsubsec:AIGCgovresponse}, many platforms have begun to adopt disclosure and labeling strategies for AI-generated content. This has quickly become one of the most intuitive and widely enforced governance measures in response to recent content ecology shifts in the surge of AI-generated content. Platforms typically justify these practices in the name of transparency: mainstream platforms frame AI labels as tools to address misleading or inauthentic content in the age of generative AI, while some creativity-focused platforms additionally position them as mitigating ownership and authorship controversies~\cite{}. However, current efforts on AI-generated content labeling are either binary---requiring the disclosure or auto-labeling whether the content is AI-generated or not~\cite{}, or present detailed information from the provenance once detected~\cite{}. These designs largely mirror the existing logic of misinformation and manipulative media governance, where fact-checking and flagging are used to mark content as either factual or potentially deceptive~\cite{}.

Yet, AI-generated content should not be treated as a binary category but as a spectrum. Generative AI can now be involved at every stage of the creative process, such as ideation, drafting, editing, and fully AI generation~\cite{}, leading to different outcomes under different extents of AI participation. To date, however, only a small number of platforms have begun to incorporate such nuance into their AI labeling policies~\cite{}. Additionally, the disclosure of AI-generated content causes ``algorithmic hate''---labeling content as AI-generated could largely compromise user-perceived~\cite{} and algorithm-rated~\cite{} content quality, usefulness, and veracity, even if the underlying content is human-made~\cite{}. Adding the fact that AI in content creation is non-binary, this raises an open question on what kind of AI participation should be disclosed, where transparency can trade off creator rights. 

The problem here is further complicated by mislabeling in platforms' auto-labeling practice, where AI-generated content is not labeled while human-made content is marked as AI-generated~\cite{}. This is perhaps unsurprising given the common failures in AI-generated content detection~\cite{}, as well as the vulnerability of provenance and watermark under malicious attacks~\cite{}. Worse, many platforms lack both clear pipelines for user reports of AI-generated content not being labeled, and appeal mechanisms for creators whose content is incorrectly labeled as AI-generated~\cite{}. 

As such, although disclosure and labeling can play an important role in improving community transparency and trust under the high-volume AI-generated content, current implementations are often coarse, arbitrary, and asymmetrically burden creators. Rather than a straightforward transparency measure, AI-generated content labels themselves can then cause contested harm.

% - Ambiguity on rules/standard (e.g., it is hard to draw the line between high quality, low quality, and spam; the definition between fully-AIGC and AI-assisted when defining ownership)

\subsection{``Harms'' are Implicit and Gradual}
\label{subsec:implicit}
 [opening paragraph TBD]

\subsubsection{Invisible Damaging on Users}
The surge of AI-generated content in online spaces could affect users’ everyday cognition and experience in content consumption, where damage is often diffuse and difficult to notice. As AI-generated content keeps flooding in online spaces, it could subtly reshape public speech~\cite{}, aesthetic norms~\cite{}, and people's expectations on what counts as good or authentic~\cite{}. Although these shifts are not necessarily negative, they still introduce obvious threats on user mental model. For example, it has been shown that co-writing with AI can shift writers' opinions in the final outcome, especially when the AI is opinionated or biased~\cite{}. Considering the strong persuasive effect of AI-generated text, it might eventually reshape, or even polarize, the opinions of the general public through widespread online distribution~\cite{}. 

The prevalence of AI-generated content in online spaces has also added additional cognitive burdens to users in daily content consumption. As the presence of AI-generated content keeps compromising user trust and belief in content authenticity, they have become frequently skeptical of whether a piece of content they encounter is human-made or AI-generated~\cite{}. Efforts in navigating AI-generated content in response to the skepticism, furthermore, pose cognitive burdens to users~\cite{}. Such actions are especially burdensome for people with extra accessibility needs~\cite{}, and can be complicated by the failures of disclosure and labeling AI-generated content as discussed above. 

These invisible damages are further intensified by the fact that AI-generated content is, in practice, hard to identify. There has been extensive evidence showing that fully AI-generated text and media are hard to distinguish from human works by people~\cite{}. In the context of online spaces, there is no doubt that viewers can have more difficulty distinguishing AI engagement in content due to the various formats and extents of AI participation in content creation~\cite{}. Young children and older adults, who have been regarded as vulnerable to explicit harms caused by AI-generated content, such as AI-generated misinformation and scam~\cite{}, may be even more at risk due to invisible damages due to limited literacy about what AI-generated content is and what threats it may pose.

In sum, users may experience harm from AI-generated content without ever recognizing that harm has occurred, or even realizing that AI was involved in the first place. And in these cases, the AI-generated content itself may not be inherently harmful.

\subsubsection{Disruption of Community}
Compromise community trust indirectly: Viewers' varied acceptance and mixed feelings on AIGC. Creators can be suspected of using AI and be blamed (perceptual harms)~\cite{}. Even in AI-generated content-dominant communities, creators using AI for creativity can also be blamed for distributing deceptive or inauthentic information~\cite{}. These factors could lead to controversial comments and trigger hateful speech.

erosion of the value of the platform: many platforms have suffered from AI-generated content in their epistemic. In general, AI-generated content is inherently treated as low quality, low value, and inauthetic~\cite{}. As such, user-perceived community value and conversation quality decrease when using generative AI in the interaction of the online community~\cite{}. AI-generated content has already shifted the norms of the community negatively. Low-quality AI slop especially hurt for a professional or knowledge-sharing platform on descreased visibility of useful content.  A study on an art community found that with the introduction of AI-generated artwork, the whole community becomes less sensitive to the copyright of creation. Lead to user migration---for example, StackOverflow has experienced a drastic decreasing on active users.


\subsection{``Harms'' are Unintentional}
The producer of problematic content is usually malicious
But generative AI can be misused by creators in content creation (no intention to cause harm, but harm happens)
Note: 1) trade-offs between entertainment and the harm of deepfakes also existed before the GenAI product surge. But this is not that much – since making high-quality deepfakes requires a high cost, most entertainment is “cheapfake” that is easy to distinguish. 2) Creator has limited literacy and guide on responsibly use AI in content creation. For example, in online communities, disclosing AIGC helps, but the problem is creators may not know they should disclose, when should they disclose, and how they can process the disclosure responsibly




% Note: basically two problems: 1. harm is dynamical, 2. the boundary of issues are vague




