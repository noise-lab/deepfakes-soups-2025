\section{Why We Need To Extend Current Governance Approaches?}
%\mc{maybe heading is why we need extension of current approaches}
\label{sec:discuss}
As reviewed in Section \ref{sec:history}, the governance of AI-generated content has been largely coalesced around mitigating deepfakes used to facilitate high-stakes online harms, and address how large generative models aggregate harmful content and manage information authenticity concerns. Here, generative AI is treated as an amplifier of existing, explicit online harms. Platform governance actions usually follow a logic similar to typical content moderation: identifying specific harm categories, locating the problematic content and bad actors, then responding by taking down content, labeling such content, or penalizing users who post it.

While current governance approaches have covered severe consequences of AI-generated content abuses, such as non-consensual intimate media and scams, they may be insufficient for addressing the broader challenges arising from the spread of online AI-generated content now and in the future.
The landscape of the problem has shifted: deepfakes and overt abuse of AI-generated content represent only the tip of the iceberg. The more profound and large-scale challenges lie beneath the surface---usually in the pervasive integration of generative AI into everyday content creation, sharing, and digital communication. The negative consequences that arise here may not fit traditional online harm categories, but still pose systemic socio-technical challenges to user online experiences, creator rights on content ownership and credits, and community trust and values. We argue that these challenges either fall beyond the scope of the current governance frameworks or cannot be well-addressed through the current governance logic.

Below, we first discuss how AI-generated content changes the paradigm of what it means to create, distribute, and consume content in online spaces, which will continue to evolve because of the transition to increasing uses of generative AI (Section \ref{subsec:shift}). Building on this context, we discuss how this shift may give rise to new challenges that may not be well-addressed by existing governance approaches: causing controversies on defining content to be moderated or govern (Section \ref{subsec:contested}), and leading to implicit and gradual erosion of user values and communities (Section \ref{subsec:implicit}). 

\subsection{The Paradigm Shift: From Malicious-Focused to Everyday Content Creation and Communication}
\label{subsec:shift}
Before the surge of large generative models and generative AI products, the production and distribution of AI-generated content in online spaces were largely limited in their scope, dominated by malicious rather than creative purposes. Although GANs were used in creative digital arts since their development, they were mostly adopted by experts in professional settings, where controversies often emerged in real-world commercial uses rather than through widespread online distribution~\cite{epstein2020gets}. Some online content creators also adopted GANs to create fun or satirical content, although these creations were mostly low-quality cheapfakes~\cite{westerlund2019emergence}. Beyond this, most AI-generated content in online spaces before the wide adoption of large generative models was deemed to be harmful deepfakes, mostly created by bad actors~\cite{westerlund2019emergence}. For example, a report showed that by 2019, over 90\% of online deepfake videos were pornographic~\cite{oxford2019deepfakes}.

Such a malicious-centric online distribution pattern beforehand was not coincidental, as the technical barrier to creating high-quality AI-generated content was high, making it inaccessible to most people. The innovation in large generative models, then, suddenly shifted generative AI technology from a high-technical, professional tool to an easy-to-use everyday application. To date, generative AI has become a common content production tool for everyone, widely used in areas like creative writing, art, and music by both professional creators and general users~\cite{zhou2024generative, wan2024felt, deruty2022development}. More broadly, the generation capability of generative AI has been applied beyond facilitating creativity. With the increasing adoption of generative AI in daily life, AI-mediated communication, where the communicator leverages AI to modify, augment, and generate messages, has become the dominant pattern in digital communication. The concept of AI-mediated communication was first proposed by Hancock and colleagues in 2020, when AI techniques were most widely used to facilitate text-based communication through grammar check, auto-reply, and auto-complete a message~\cite{hancock2020ai}. The prevalence of generative AI later expanded AI-mediated communication to AI-generated messages, human-AI co-writing, and multimodal-based communication~\cite{li2024value, fu2025should}.  

The everyday generative AI technology usage nowadays has also led to a significant shift in the online information ecosystem, where both the volume of AI-generated content and the composition of information have changed. Online platforms have witnessed a surge of volume in AI-generated content---spanning a variety of media such as text, image, and art~\cite{liang2025widespread,matatov2024examining,wei2024understanding,sun2025we,guo2025exploring}. Generative AI has been widely adopted by users to construct profiles, write comments, and online reviews~\cite{shahid2025examining, jakesch2019ai, barkallah2026wanted}.  Online content creators also increasingly use generative AI to facilitate content creation. For example, studies have found video creators in social media increasingly integrating generative AI in their workflow from ideation to video editing~\cite{kim2024unlocking, lyu2024youtube}. Some other researchers also found that generative AI has incentivized many new creators to produce and share their creations~\cite{matatov2024examining, wei2024exploring}. Meanwhile, online platforms themselves also proactively integrate AI-generated content as a part of their ecosystem. Many online platforms hosting user-generated content have provided generative AI tools for users and creators, encouraging them to make and share high-quality, creative content empowered by AI~\cite{gao2026governance}. Platforms also use generative AI to create platform-generated content like AI search summaries, to facilitate the content consumption of users~\cite{gao2026governance, gao2025does}. 

Undoubtedly, the risks that come from the malicious production of AI-generated content have also been largely intensified by the introduction of advanced generative AI. However, the paradigm shift of AI-generated content production and distribution in the era of generative AI---where AI components are increasingly and invisibly integrated into the whole online information ecosystem---has contributed to a broader range of challenges. We discuss how these emerging challenges call for an extension of the existing governance framework in the rest of this section.

\subsection{Emerging and Existing Harms with Contested Boundaries}
\label{subsec:contested}
In the current governance framework, problematic AI-generated content is often treated as having a clear-cut harm and violating existing policies. However, as AI-generated content keeps flooding online spaces, controversies around AI-generated content itself give rise to harms with contested boundaries, where what counts as ``problematic'', for whom, and under what conditions is difficult to define. Particularly, issues such as ownership and quality have become more salient under large-scale generative AI adoption, yet remain difficult to govern through existing policies. Many platforms, however, continue to fall back on existing mechanisms (e.g., copyright policy or spam detection), with only a minority articulating AI-specific alternatives~\cite{gao2026governance}. Meanwhile, one-size-fits-all interventions, such as labeling policies that treat AI-generated or not as a binary category, can be controversial in practice.

Below, we first discuss the governance challenges in two cases:  content ownership and low-quality AI-generated content. We then discuss how the current governance approach of labeling AI-generated content---typically tackling inauthenticity and ownership debates---can introduce trade-offs. 


\subsubsection{The Debatable Ownership of AI-Generated Content}
\label{subsubsec:ownership}
In creator economies, copyright and ownership map directly onto platform decisions around promotion, monetization, and creator rewards~\cite{cosen2024new, chu2022behind}. Now, such problems move beyond clear-cut content theft or using copyrighted materials to the ownership controversies of AI generation itself. As much content incorporates AI components, it becomes harder to determine who should be credited for a given piece of content and to what extent. The growing trend of using low-quality AI-generated content produced with minimal human effort to gain audience or income, as pointed out in recent research~\cite{wei2024understanding,guo2025exploring, zhang2025democratizing, diresta2024spammers}, further intensifies concerns about how creators' human labor should be fairly accounted. 

Debates about ownership and authorship of AI-generated work predate the pervasion of generative AI as an everyday application~\cite{epstein2020gets}. And now, there is a broad recognition that existing copyright law is not comprehensive enough to address AI-generated content~\cite{samuelson2023generative}. Researchers have proposed several ideas for calculating credits for AI-generated content made in the generation phase, such as by estimating human controls in content generation~\cite{10.1145/3715336.3735683} and how different training data contribute to the output~\cite{lee2024talkin}. However, there is still no consensus on how to define ownership of AI generation until now, and such controversy is partly compensated for by generative AI services through incorporating content provenance~\cite{C2PA2025}.

Additionally, public perceptions of AI-generated content ownership could diverge from technical or legal framings. Since the rise of generative AI in content creation, researchers have kept examining the perception divergences of ownership among different groups. Prior works found that viewers tend to devalue human contribution in AI-generated work~\cite{lima2025public, 10.1145/3706598.3713522}, and attribute credits to the AI model and the training data provider~\cite{lima2025public}. Research on creative workers whose work contributes to training data, including writers, artists, and code developers, has revealed a need for frameworks to recognize their contributions and compensation in generative AI for content creation~\cite{Lovato_Zimmerman_Smith_Dodds_Karson_2024, kyi2025governance, gero2025creative}. Creators of AI-generated content, however, were found in some cases to overestimate their ownership of human-AI co-creation, and even attribute all credits to themselves rather than AI (i.e., AI ghostwriter effect)~\cite{kim2025s, 10.1145/3637875}. These divergences matter most in online platforms, where ownership is operationalized through ranking and monetization decisions and can cause debates among users, content creators, and training data contributors.
\mc{would be good to cite a well known case like Grimes and her being ok with people using AI to use her style vs others who are not ok with it, makes the arguments less abstract, in general a few high profile examples would be good to elevate and enhance what you're saying in the paper} \lan{TBD - I'm thinking about these research studies, including the example of Grimes, are actually not limited to online platforms or online spaces? I'm trying to reframe this paragraph as evidence coming from previous research, and why these conclusions imply that things can get worse in online platforms/online spaces}

Under this context, platforms face difficulties about how to allocate algorithmic visibility, incentives, and financial rewards among creators who use AI to different degrees and for different purposes. To date, however, generative AI services and many online communities continue to rely on traditional copyright law (e.g., DMCA) as the primary mechanism for ownership disputes~\cite{gao2025cannot,gao2026governance}. A study on 40 social media platforms found only six platforms made alternative rules on monetizing AI-generated content, with three outright banning AI-generated content from monetization or promotion and claiming preserve human originality~\cite{gao2026governance}. However, these binary governance strategies are likely to fail under the circumstances where the use of generative AI in content creation is becoming ubiquitous.

\subsubsection{The Unclear Boundary and Value of Low-Quality, Low-Effort AI-Generated Content}
\label{subsubsec:quality}
Another governance challenge lies in recent debates over the high volume, low quality AI-generated content made with low efforts in online spaces, often cited as ``AI slop''~\cite{Gross2025AISlopAP, mahdawi2025slop, hoffman2024first}. According to some recent news articles, public blame around AI slop resembles that attributed to traditional spam posts: it can flood feeds and degrade users' perceived information quality~\cite{mahdawi2025slop, hoffman2024first}. And indeed, research on Facebook has shown the increasing adoption of generative AI by spammers for audience growth through creating massive low-quality AI-generated content~\cite{diresta2024spammers}. However, unlike classic spam, which is typically defined by malicious or opportunistic intent with overlapping categories like scam and phishing~\cite{inuwa2018detection, ferrara2019history}, many low-effort AI-generated content creators lack a clearly malicious purpose---but just for creativity. 

Moreover, low-effort AI-generated content is not uniformly valueless from the perspective of users and communities. AI-generated writings, videos, and images can be perceived as entertaining, playful, or creatively inspiring, despite being produced with minimal human effort~\cite{kommers2026ai, park2024ai}. An experimental study shows that users appreciate creators who produce low-effort AI-generated content at the same level as, or even more than, human creators~\cite{park2024ai}. The phenomenon of AI slop, as already argued by some researchers, may open up an opportunity to supply the high cultural and economic demand of content consumption with its own aesthetic value~\cite{kommers2025slop}.

Under these conditions, it becomes difficult to draw a clear line between low-quality AI content that undermines user experience and a contributive one. For platforms, adopting a strict governance stance against AI slop suppresses legitimate forms of participation and creativity, while a more permissive approach may exacerbate concerns about information ecosystem pollution. This tension makes the governance of low-quality AI-generated content inherently contested rather than a straightforward harm. And until now, only a few online platforms have responded to this problem by making new policies~\cite{gao2026governance}.%\mc{so the point here is when does this become harmful and why? again having example would help} \lan{will add a sentence like section 4.2.1 on neither this problem is widely addressed, nor it can be addressed through a clear-cut moderation (like remove, flag, downgrade), because of the high volume and unclear boundary}


\subsubsection{The Trade-Offs of Labeling AI-Generated Content}
\label{subsubsec:labelingtradeoff}
As reviewed in Section \ref{subsubsec:AIGCgovresponse}, many platforms have begun to adopt disclosure and labeling strategies for AI-generated content. This has quickly become one of the most intuitive and widely enforced governance measures in response to the recent surge of AI-generated content. Platforms typically justify these practices in the name of transparency: mainstream platforms frame AI labels as tools to address misleading or inauthentic content in the age of generative AI, while some creativity-focused platforms additionally position them as mitigating ownership and authorship controversies~\cite{gao2026governance}. However, current efforts on AI-generated content labeling are either binary---requiring the disclosure or auto-labeling whether the content is AI-generated or not~\cite{gao2026governance}, or present detailed information from the provenance once detected~\cite{feng2023examining}. These designs largely mirror the existing logic of misinformation and manipulative media governance, where fact-checking is used to flag or label content as either factual or potentially deceptive~\cite{martel2024fact, martel2023misinformation}.

Yet, AI-generated content should not be treated as a binary category but as a spectrum. Generative AI can now be involved at every stage of the creative process, such as ideation, drafting, editing, and fully AI generation~\cite{moruzzi2025content, kim2024unlocking}, leading to different outcomes under different extents of AI participation. To date, however, only a small number of platforms have begun to incorporate such nuance into their AI labeling policies~\cite{gao2026governance}. Additionally, the disclosure of AI-generated content can cause a bad impression regardless of the content itself---many controlled experiments show that labeling content as AI-generated could largely compromise user-perceived~\cite{moruzzi2025content, cheong2025penalizing, chen2025ai} and algorithm-rated~\cite{cheong2025penalizing} content quality, usefulness, and veracity compared to the same content without labels, even if the underlying content is human-made~\cite{altay2024people}. Adding the fact that AI in content creation is non-binary, this raises an open question on what kind of AI participation should be disclosed, where transparency can trade off creator rights. 

The problem here is further complicated by the widely reported failures in platforms' auto-labeling practices in the wild, where AI-generated content is not labeled while human-made content is erroneously marked as AI-generated~\cite{mantzarlis_dutta_2025, kuiperslabeling}. This is perhaps unsurprising given the common ineffectiveness in AI-generated content detection~\cite{boutadjine2025human, weber2023testing}, as well as the vulnerability of provenance and watermark under malicious attacks~\cite{jiang2023evading}. Worse, recent research found that many platforms lacked both clear pipelines for user reports of AI-generated content not being labeled, and appeal mechanisms for creators whose content is incorrectly labeled as AI-generated until recently~\cite{gao2026governance}. As inaccuracy in manipulative media labeling has already been shown to lead to user skepticism in factual information and overtrust in misinformation~\cite{sherman2021designing}, such mislabeling in AI-generated content may similarly burden users and damage creators.

As such, although disclosure and labeling can play an important role in improving community transparency and trust under the high-volume AI-generated content, current implementations are often coarse, arbitrary, and asymmetrically burden creators. Rather than a straightforward transparency measure, AI-generated content labels themselves can then cause controversy and harm to creators and users.

% - Ambiguity on rules/standard (e.g., it is hard to draw the line between high quality, low quality, and spam; the definition between fully-AIGC and AI-assisted when defining ownership)

\subsection{Implicit Harms from the Widespread Distribution of AI-Generated Content}
\label{subsec:implicit}
Since the widespread adoption of generative AI, researchers across HCI, social computing, and communication and media studies have increasingly examined how the AI-generated content flood negatively affects user experience when interacting with online content, as well as the norms and values of online communities. Many of these concerns, however, are only partially captured by current governance approaches. Below, we discuss how these negative affect results in challenges in online trust and safety, and how current governance approaches may be insufficient to address them.

\subsubsection{Invisible Damages on Users}
\label{subsubsec:invisibledamage}
The surge of AI-generated content may gradually reshape users’ cognition and behavior through routine exposure to online information. Damages that arise here are usually not immediately destructive, but often cumulative and unfold without users’ awareness. Research has found that the flood of AI-generated content has subtly changed public speech~\cite{Gross2025AISlopAP,chen2025synthetic} and aesthetic norms~\cite{Gross2025AISlopAP, kommers2026ai}. Although these shifts are not necessarily negative, they still introduce obvious threats to a user's mental model and long-term welfare. For example, it has been shown that co-writing with AI can shift writers' opinions in the final outcome, especially when the AI is opinionated or biased~\cite{jakesch2023co,shahid2025examining}. Considering the strong persuasive effect of AI-generated text~\cite{bai2025llm, salvi2025conversational}, it might eventually shape opinion formation in systematic ways through widespread online distribution. Meanwhile, researchers have raised concerns about the potential developmental harms for children exposed to AI-generated content environments. For example, a study reports an association between high technology use and poorer performance among young children in distinguishing authentic content from AI-generated one~\cite{langer5678206children}.

The prevalence of AI-generated content has also added additional cognitive burdens to users' routine interaction with online content. As the presence of AI-generated content keeps compromising user trust and belief in content authenticity, users can become frequently skeptical of whether a piece of content they encounter is human-made or AI-generated. Efforts in navigating AI-generated content in response to the skepticism, furthermore, pose cognitive burdens to users~\cite{eissa2025influence}. Such verification actions are especially burdensome for people with extra accessibility needs due to the inapplicability of visual signal~\cite{ide2025signals}, and can be complicated by the trade-off of disclosure and labeling AI-generated content as discussed above. 

These chronic damages are further intensified by the fact that AI-generated content is, in practice, hard to identify. There has been extensive evidence showing that fully AI-generated text and media are hard for people to distinguish from human works in experimental contexts~\cite{boutadjine2025human, ha2024organic, lu2023seeing, zhou2023synthetic, park2024ai}. In the context of online spaces, viewers can have more difficulty distinguishing AI-generated content due to the various formats and extents of AI participation in content creation. Research has also shown that users tend to rely on flawed heuristics in identifying online AI-generated content~\cite{jakesch2023human, miskolczi2025illusion}. Young children and older adults, who have been regarded as vulnerable to explicit harms like AI-generated misinformation and scams~\cite{zhai2025hear, lao2025everyday}, may be even more at risk due to invisible damages like opinion polarization and cognitive burden due to limited literacy and awareness about what AI-generated content is, what threats it may pose, and how to identify them.

%\mc{this next sentence is most clear about the harm type here and why it is invisible} 
In short, users may experience harm from AI-generated content without ever recognizing that harm has occurred, gradually damaging their cognition. In these cases, the AI-generated material that causes harm may not be inherently harmful. Instead, the damage often arises from the general prevalence of AI-generated content online. We thus regard these invisible damages lies outside the scope of the current governance framework, which typically targets single items that cause potential harm.

\subsubsection{Disruption of Community}
\label{subsubsec:communityvalue}
A further set of harms emerges at the community level, where the distribution of AI-generated content can indirectly and gradually compromise trustworthiness among users and creators. For many people, AI-generated content is inherently low quality, low value, and inauthentic~\cite{rae2024effects, ragot2020ai}. As such, creators using AI only for creative purposes have the risk of being blamed for producing spammy, deceptive, or inauthentic information~\cite{moruzzi2025content}. At the same time, as skepticism towards online information authenticity keeps growing, many creators who use no, or minimal AI in their workflow have been suspected of creating low-effort AI-generated content~\cite{matatov2024examining}. Prior work has found that writers with specific demographics are more likely to be suspected of using AI in their writings~\cite{Kadoma2024GenerativeAA}. When considering the context of online spaces, such phenomena could have the potential to introduce bias towards the online community. A study on art subreddits found that these dynamics have led to controversial and negative comments in the communities over time~\cite{matatov2024examining}.

Beyond interpersonal trust, AI-generated content can also undermine the perceived value and epistemic function of platforms. While generative AI has shown its value in fostering user engagement and new creator participation~\cite{moller2025impact, matatov2024examining, wei2024exploring}, it is found that user-perceived community value and conversation quality usually decrease at the same time~\cite{moller2025impact, eissa2025influence, lloyd2025there}. In professional or knowledge-sharing platforms like StackOverflow, people have already witnessed a migration of users to generative AI tools for question-asking~\cite{zhou2024examining, burtch2023consequences, kabir2024stack}. The flood of AI-generated content, as researchers found, also decreases the volume of high-quality content regardless of an increase in the overall content volume~\cite{li2025impacts, burtch2023consequences}. A study of DeviantArt, a creative community, similarly suggests that the introduction of AI-generated artworks have weaken shared norms around copyright, originality, and mutual emotional support among users~\cite{guo2025exploring}. 
%\mc{stackoverflow?}

Taken together, these trends suggest that widespread use of AI-generated content can disrupt not only individual experiences but also the trust, norms, and epistemic roles of entire communities. Such effects have been shown to be difficult to address within incident-based, harm-centric governance frameworks. For example, even though StackOverflow has banned any AI-generated content in its policies, the erosion of community value still happened as discussed above. A study also found unintended consequences of banning AI-generated content in StackExchange: an increase in human-made low-quality answers and a potential decline in new questions~\cite{wang2024can}. %\mc{love the examples from Stack overflow etc}

%\subsection{Unintentional Harms} \mc{is this section done?}
%\label{subsec:unintent}
%In online platform content moderation, problematic content is often framed through an adversarial lens in which bad actors are central. Even in misinformation contexts where distributors may lack malicious intent, the dominant threat model still foregrounds deceptive actors and the tendency of misleading content to circulate within homogeneous, polarized groups~\cite{del2016spreading}. AI-generated content shares some similarity with misinformation, yet it further expands the space of unintentional harms that arise from content creators' widespread adoption of generative AI for routine content creation, which may then be recommended across everyone's feeds. 

%To some extent, the trade-off between entertainment and online harm has existed prior to the recent surge of generative AI products. For instance, misleading content produced by satire or entertainment purpose have long circulated online. However, before the wide adoption of generative AI, much of the entertainment-oriented manipulated media remained ``cheapfakes'' that viewers could readily recognize~\cite{westerlund2019emergence}. By contrast, today’s generative AI tools' capability of producing realistic media could make inadvertent deception and misleading more likely even in everyday creative workflows. Such a shift calls for a more urgent need to better address the unintentional harms of AI-generated content.

%The unintentional harms of AI-generated are further amplified by the weak guidance on responsible AI use in content creation. A recent study on short video creators reveals their urgent needs for platforms to provide tools and guidelines on using AI to create `ethical and responsible’ content, particularly to avoid unintentional copyright violation and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. However, studies reveal that to date, only a few platforms have offered concrete guidelines on how generative AI should be responsibly used in content creation beyond the guidance on AI-generated content disclosure~\cite{gao2026governance}.






% Note: basically two problems: 1. harm is dynamical, 2. the boundary of issues are vague




