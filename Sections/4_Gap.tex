\section{Why We Need Extension of Current Governance Approaches?}
%\mc{maybe heading is why we need extension of current approaches}
\label{sec:discuss}
As reviewed in Section \ref{sec:history}, the governance of AI-generated content has been largely coalesced around mitigating explicit harm. From deepfake manipulation to inauthentic AI-generated content, platform responses have followed an incident-driven paradigm---identifying specific harm categories, problematic content, and bad actors, then responding through content moderation enforcement.

While this harm-dominant trajectory is indeed necessary, particularly given the severe consequences of high-stakes abuses such as non-consensual intimate media and scams, it may be insufficient for addressing the broader governance needs of AI-generated content now and in the future.
The landscape of the problem has shifted: deepfakes and overt abuse of AI-generated content represent only the tip of the iceberg. The more profound and large-scale disruption lies beneath the surface---usually in the routine, non-malicious, yet pervasive integration of generative AI into everyday digital content production and communication. These uses may not fit traditional online harm categories but still pose systemic challenges to user experiences, creator rights on content ownership and credits, and community trust and values, which fall beyond the scope of the current harm-dominant governance frameworks around AI-generated content.

Below, we first discuss how AI-generated content changes the paradigm of what it means to create, consume, and distribute content in online spaces---and how it will continue to evolve---because of the transition to increasing uses of generative AI (Section \ref{subsec:shift}). Building on this context, we then discuss how shifts in the online content ecology may give \mc{needs explanation for what these things mean } rise to contested (Section \ref{subsec:contested}), implicit and gradual (Section \ref{subsec:implicit}), and unintentional (Section \ref{}) harms that may not be well-addressed by conventional governance approaches targeting explicit harms.

\subsection{The Paradigm Shift: From Malicious-Focused to Everyday Content Production and Communication}
\label{subsec:shift}
Before the surge of large generative models and generative AI products, the production and distribution of AI-generated content in online spaces were largely limited in their scope, dominated by malicious rather than creative purposes. Although GANs were used in creative digital arts since their development, they were mostly adopted by experts in professional settings, where controversies often emerged in real-world commercial uses rather than through widespread online distribution~\cite{epstein2020gets}. Some online content creators also adopted GANs to create fun or satirical content, although these creations were mostly low-quality cheapfakes~\cite{westerlund2019emergence}. Beyond this, most AI-generated content in online spaces before the wide adoption of large generative models was deemed to be harmful deepfakes, mostly created by bad actors~\cite{westerlund2019emergence}. For example, a report showed that by 2019, over 90\% of online deepfake videos were pornographic~\cite{oxford2019deepfakes}.

Such a malicious-centric online distribution pattern beforehand was not coincidental, as the technical barrier to creating high-quality AI-generated content was high, making it inaccessible to most people. The innovation in large generative models, then, suddenly shifted generative AI technology from a high-technical, professional tool to an easy-to-use everyday application. To date, generative AI has become a common content production tool for everyone, widely used in areas like creative writing, art, and music by both professional creators and general users~\cite{zhou2024generative, wan2024felt, deruty2022development}. More broadly, the generation capability of generative AI has been applied beyond facilitating creativity. With the increasing adoption of generative AI in daily life, AI-mediated communication, where the communicator leverages AI to modify, augment, and generate messages, has become the dominant pattern in digital communication. The concept of AI-mediated communication was first proposed by Hancock and colleagues in 2020, when AI techniques were most widely used to facilitate text-based communication through grammar check, auto-reply, and auto-complete a message~\cite{hancock2020ai}. The prevalence of generative AI later expanded AI-mediated communication to AI-generated messages, human-AI co-writing, and multimodal-based communication~\cite{li2024value, fu2025should}.  

The paradigm shift in generative AI technology usage nowadays has also led to a significant change in online content ecology. Online platforms have witnessed a surge of volume in AI-generated content---spanning a variety of media such as text, image, and art~\cite{liang2025widespread,matatov2024examining,wei2024understanding,sun2025we,guo2025exploring}. Generative AI has been widely adopted by users to construct profiles, write comments, and online reviews~\cite{shahid2025examining, jakesch2019ai, barkallah2026wanted}.  Online content creators also increasingly use generative AI to facilitate content creation. For example, video creators in social media are increasingly integrating generative AI in their workflow from ideation to video editing~\cite{kim2024unlocking, lyu2024youtube}. Generative AI has also incentivized many new creators to produce and share their creations~\cite{matatov2024examining, wei2024exploring}. Meanwhile, online platforms themselves also proactively integrate AI-generated content as a part of their ecosystem. Many online platforms hosting user-generated content have provided generative AI tools for users and creators, encouraging them to make and share high-quality, creative content empowered by AI~\cite{gao2026governance}. Platforms also use generative AI to create platform-generated content like AI search summaries, to facilitate the content consumption of users~\cite{gao2026governance, gao2025does}. 

Undoubtedly, the risks that come from the malicious production of AI-generated content have also been intensified by the introduction of generative AI as an everyday technology. However, the paradigm shift of AI-generated content production and distribution---where AI components are increasingly and invisibly integrated into the whole online content ecology---has contributed to a broader range harms. We discuss how these harms are different from those of the malicious, inherently problematic AI-generated content in the rest of this section.

\subsection{Contested Harms Raised from AI-Generated Content and Harm-Centric Governance}
\label{subsec:contested}
In the current harm-centric \mc{probably want to move away from harm-centric since you're still outlining other harms in this section - i think you're saying there are other harms not being addressed not that harm-centric is bad, to me you're saying there are some harms nobody is addressing in depth and that AI is creating other new harms like what it means to own and make content or whether we can tell if content is human or AI made etc} \lan{let's discuss on this--I'm thinking about change a wording on this. What I want to say on current governance approaches is (copied from intro, but I'm still thinking about a better framing): Yet, platform governance remains largely confined to explicit harms and follows an incident-driven logic: identifying and moderating AI-generated content that likely to cause direct harm,
where the underlying threat model—attacker, victim, harm,
and consequence—is typically clear and well-defined.} governance framework, the harms caused by problematic AI-generated content are often treated as clear-cut. 
However, as AI-generated content keeps flooding online spaces, controversies around AI-generated content itself give rise to potential harms whose boundaries are difficult to define. Issues such as ownership and content quality---already under-emphasized in existing governance~\cite{gao2026governance}---become even more salient in this context. Neither can these concerns be fully addressed by the harm-centric  \mc{same comment as above} governance strategy, nor can a one-size-fits-all governance approach following a harm-centric \mc{same} logic be fair enough for creators and users. 

Below, we first discuss contested \mc{define what contested means} harms on ownership and quality of AI-generated content that arise from the nature of such content and its wide distribution. We then discuss how the current approach to labeling AI-generated content---typically for improving community transparency by tackling inauthenticity and ownership debates---can introduce trade-offs and cause harms. 


\subsubsection{The Debatable Ownership of AI-Generated Content}
Controversies around ownership and authorship of AI-generated content have become salient in online communities. %Generally, unclear ownership can lead to intellectual property disputes and complicate the commercial use of a work~\cite{}. 
In creator economies, copyright and ownership map directly onto platform decisions around promotion, monetization, and creator rewards~\cite{cosen2024new, chu2022behind}. As generative AI is increasingly used in every aspect of content creation, it becomes harder to determine who should be credited for a given piece of content and to what extent. The growing trend of using AI-generated content produced with minimal effort to gain influence or income, which has already been pointed out in recent research~\cite{wei2024understanding,guo2025exploring, zhang2025democratizing, diresta2024spammers}, further intensifies concerns about how creators' human labor should be fairly treated. 

Debates about ownership and authorship of AI-generated work predate the pervasion of generative AI as an everyday application~\cite{epstein2020gets}. And now, there is a broad recognition that existing copyright law is not comprehensive enough to address AI-generated content~\cite{samuelson2023generative}, even though generative AI services and many online communities continue to rely on traditional copyright law as the primary mechanism for ownership disputes~\cite{gao2025cannot,gao2026governance}. Researchers have proposed several ideas for calculating credits for AI-generated content made in the generation phase, such as by estimating human controls in content generation~\cite{10.1145/3715336.3735683} and how different training data contribute to the output~\cite{lee2024talkin}. However, there is still no consensus on how to define fair ownership until now.

Additionally, public perceptions of AI-generated content ownership could diverge from technical or legal framings. And even worse, perception divergences are widely different among different groups. Viewers tend to devalue human contribution in AI-generated work~\cite{lima2025public, 10.1145/3706598.3713522}, and credit the AI model and the training data provider~\cite{lima2025public}. Human creators whose works contribute to the training data have long called for recognition of their credits and compensation~\cite{Lovato_Zimmerman_Smith_Dodds_Karson_2024, kyi2025governance, gero2025creative}. Creators of AI-generated content, however, often overestimate their ownership of human-AI co-creation, and sometimes even attribute all credits to themselves rather than AI (i.e., AI ghostwriter effect)~\cite{kim2025s, 10.1145/3637875}. \mc{would be good to cite a well known case like Grimes and her being ok with people using AI to use her style vs others who are not ok with it, makes the arguments less abstract, in general a few high profile examples would be good to elevate and enhance what you're saying in the paper} \lan{TBD}

Under this context, platforms face difficulties about how to allocate algorithmic visibility, incentives, and financial rewards among creators who use AI to different degrees and for different purposes. A few platforms have responded by outright banning AI-generated content from monetization or promotion, citing the reason as preserving human originality~\cite{gao2026governance}. However, these binary governance strategies are likely to fail under the circumstances where the use of generative AI in content creation is becoming ubiquitous.

\subsubsection{The Unclear Boundary and Value of Low-Quality AI-Generated Content}
Another site of contested harm \mc{what is contested } lies in debates over the high volume, low quality AI-generated content made with low efforts in online spaces, often cited as ``AI slop'' recently~\cite{Gross2025AISlopAP, mahdawi2025slop}. On the surface, the blame around AI slop resembles that attributed to traditional spam posts: it can flood feeds and degrade users' perceived information quality~\cite{inuwa2018detection}. And indeed, generative AI has been used by spammers for audience growth~\cite{diresta2024spammers}. However, unlike classic spam, which is typically defined by malicious or opportunistic intent with overlapping categories like scam and phishing~\cite{inuwa2018detection, ferrara2019history}, many low-effort AI-generated content creators lack a clearly malicious purpose. 

Moreover, low-effort AI-generated content is not uniformly valueless from the perspective of users and communities. AI-generated writings, videos, and images can be perceived as entertaining, playful, or creatively inspiring, despite being produced with minimal human effort~\cite{kommers2026ai, park2024ai}. Prior work also has shown that users appreciate creators who produce low-effort AI-generated content at the same level as, or even more than, human creators, as long as they find the creation amusing or attractive~\cite{park2024ai}. The phenomenon of AI slop, as already argued by some researchers, opens up an opportunity to supply the high cultural and economic demand of content consumption with its own aesthetic value~\cite{kommers2025slop}.

Under these conditions, it becomes difficult to draw a clear line between ``harmful'' low-quality AI content that undermines user experience and contributive one. For platforms, adopting a strict governance stance against AI slop suppresses legitimate forms of participation and creativity, while a more permissive approach may exacerbate concerns about information ecosystem pollution. This tension makes the governance of low-quality AI-generated content inherently contested rather than a straightforward harm. \mc{so the point here is when does this become harmful and why? again having example would help} \lan{will add a sentence like section 4.2.1 on neither this problem is widely addressed, nor it can be addressed through a clear-cut moderation (like remove, flag, downgrade), because of the high volume and unclear boundary}


\subsubsection{The Trade-Offs of Labeling AI-Generated Content}
As reviewed in Section \ref{subsubsec:AIGCgovresponse}, many platforms have begun to adopt disclosure and labeling strategies for AI-generated content. This has quickly become one of the most intuitive and widely enforced governance measures in response to recent content ecology \mc{define earlier} shifts in the surge of AI-generated content. Platforms typically justify these practices in the name of transparency: mainstream platforms frame AI labels as tools to address misleading or inauthentic content in the age of generative AI, while some creativity-focused platforms additionally position them as mitigating ownership and authorship controversies~\cite{gao2026governance}. However, current efforts on AI-generated content labeling are either binary---requiring the disclosure or auto-labeling whether the content is AI-generated or not~\cite{gao2026governance}, or present detailed information from the provenance once detected~\cite{feng2023examining}. These designs largely mirror the existing logic of misinformation and manipulative media governance, where fact-checking is used to flag or label content as either factual or potentially deceptive~\cite{martel2024fact, martel2023misinformation}.

Yet, AI-generated content should not be treated as a binary category but as a spectrum. Generative AI can now be involved at every stage of the creative process, such as ideation, drafting, editing, and fully AI generation~\cite{moruzzi2025content, kim2024unlocking}, leading to different outcomes under different extents of AI participation. To date, however, only a small number of platforms have begun to incorporate such nuance into their AI labeling policies~\cite{gao2026governance}. Additionally, prior research works reveal the disclosure of AI-generated content can cause ``algorithmic hate''---labeling content as AI-generated could largely compromise user-perceived~\cite{moruzzi2025content, cheong2025penalizing, chen2025ai} and algorithm-rated~\cite{cheong2025penalizing} content quality, usefulness, and veracity, even if the underlying content is human-made~\cite{altay2024people}. Adding the fact that AI in content creation is non-binary, this raises an open question on what kind of AI participation should be disclosed, where transparency can trade off creator rights. 

The problem here is further complicated by the widely reported failures in platforms' auto-labeling practices, where AI-generated content is not labeled while human-made content is erroneously marked as AI-generated~\cite{mantzarlis_dutta_2025, kuiperslabeling}. This is perhaps unsurprising given the common ineffectiveness in AI-generated content detection~\cite{boutadjine2025human, weber2023testing}, as well as the vulnerability of provenance and watermark under malicious attacks~\cite{jiang2023evading}. Worse, until recently, recent research found that many platforms lacked both clear pipelines for user reports of AI-generated content not being labeled, and appeal mechanisms for creators whose content is incorrectly labeled as AI-generated~\cite{gao2026governance}. As inaccuracy in manipulative media labeling has already been shown to lead to user skepticism in factual information and overtrust in misinformation~\cite{sherman2021designing}, such mislabeling in AI-generated content could similarly burden users and damage creators.

As such, although disclosure and labeling can play an important role in improving community transparency and trust under the high-volume AI-generated content, current implementations are often coarse, arbitrary, and asymmetrically burden creators. Rather than a straightforward transparency measure, AI-generated content labels themselves can then cause \mc{is this contested or just that it can create harms too} contested harm.

% - Ambiguity on rules/standard (e.g., it is hard to draw the line between high quality, low quality, and spam; the definition between fully-AIGC and AI-assisted when defining ownership)

\subsection{Implicit Harms from the Widespread Distribution of AI-Generated Content}
\label{subsec:implicit}
\lan{[opening paragraph TBD]}

% \mc{think earlier you can also explain content consumption etc in simpler terms i.e. what users see and consume online or distribution as what users create and distribute}
\subsubsection{Invisible Damages on Users}
The surge of AI-generated content in online spaces could affect how users think and behave in online content consumption. Damages arise here are usually not immediately destructive, but gradually and invisibly affect user. Research has found that the flood of AI-generated content has subtly change public opinion~\cite{Gross2025AISlopAP,chen2025synthetic} and aesthetic norms~\cite{kommers2026ai}. Although these shifts are not necessarily negative, they still introduce obvious threats to a user's mental model and long-term welfare. For example, it has been shown that co-writing with AI can shift writers' opinions in the final outcome, especially when the AI is opinionated or biased~\cite{jakesch2023co,shahid2025examining}. Considering the strong persuasive effect of AI-generated text~\cite{bai2025llm, salvi2025conversational}, it might eventually polarize the opinions of the general public through widespread online distribution. 

The prevalence of AI-generated content in online spaces has also added additional cognitive burdens to users in their daily content consumption online. As the presence of AI-generated content keeps compromising user trust and belief in content authenticity, users have become frequently skeptical of whether a piece of content they encounter is human-made or AI-generated~\cite{}. Efforts in navigating AI-generated content in response to the skepticism, furthermore, poses cognitive burdens to users~\cite{eissa2025influence}. Such actions are especially burdensome for people with extra accessibility needs~\cite{ide2025signals}, and can be complicated by the failures of disclosure and labeling AI-generated content as discussed above. %\mc{also harder for kids since they are growing up with this content, some kids think everything is AIgenerated now}

These chronic damages are further intensified by the fact that AI-generated content is, in practice, hard to identify. There has been extensive evidence showing that fully AI-generated text and media are hard for people to distinguish from human works~\cite{boutadjine2025human, ha2024organic, lu2023seeing, zhou2023synthetic}. In the context of online spaces, there is no doubt that viewers can have more difficulty distinguishing AI engagement in content \mc{what does AI engagement in content mean} due to the various formats and extents of AI participation in content creation~\cite{park2024ai}. Young children and older adults, who have been regarded as vulnerable to explicit harms caused by AI-generated content, such as AI-generated misinformation and scams~\cite{zhai2025hear, lao2025everyday}, may be even more at risk due to invisible damages \mc{explain what these are} due to limited literacy and cognition about what AI-generated content is, what threats it may pose, and how to identify them.

%\mc{this next sentence is most clear about the harm type here and why it is invisible} 
In short, users may experience harm from AI-generated content without ever recognizing that harm has occurred, or even realizing that AI was involved in the first place. In these cases, the AI-generated content itself that causes harm may not be inherently harmful, which lies outside the scope of the current explicit-harm-centric \mc{change from harm centric - I think in this case you're saying in some instances we dont even know or cant measure these harms since they are evolving and hard to measure or track} governance framework.

\subsubsection{Disruption of Community}
A further set of harms emerges at the community level, where the distribution of AI-generated content can indirectly and gradually compromise trustworthiness among users and creators. For many people, AI-generated content is inherently low quality, low value, and inauthentic~\cite{rae2024effects, }. As such, creators using AI only for creative purposes can be blamed for producing spammy, deceptive, or inauthentic information~\cite{}. At the same time, as skepticism towards online content authenticity  
keeps growing, many creators who use no, or minimal AI in their workflow have been suspected of creating low-effort AI-generated content~\cite{matatov2024examining,}. Prior work has found that writers with specific demographics are more likely to be suspected of using AI in their writings~\cite{Kadoma2024GenerativeAA}, a phenomenon, when considering it in the context of online spaces, could have the potential to introduce bias towards the community. These dynamics could lead to controversial comments and hateful speech on the platform~\cite{}.

Beyond interpersonal trust, AI-generated content can also undermine the perceived value and epistemic function of platforms. While generative AI has shown its value in fostering user engagement and new creators~\cite{moller2025impact, matatov2024examining, wei2024exploring}, it is found that user-perceived community value and conversation quality usually decrease at the same time~\cite{moller2025impact, eissa2025influence, }. In professional or knowledge-sharing platforms like StackOverflow, people have already witnessed a migration of users to generative AI tools for question-asking~\cite{zhou2024examining, burtch2023consequences, kabir2024stack}. The flood of AI-generated content, as researchers found, also decreases the volume of high-quality content regardless of an increase in the overall content volume~\cite{li2025impacts, burtch2023consequences}. A study of DeviantArt, a creative community, similarly suggests that the introduction of AI-generated artworks have weaken shared norms around copyright, originality, and mutual emotional support among users~\cite{guo2025exploring}. 
%\mc{stackoverflow?}

Taken together, these trends suggest that widespread use of AI-generated content can disrupt not only individual experiences but also the trust, norms, and epistemic roles of entire communities. Such effects have been shown to be difficult to address within incident-based, harm-centric governance frameworks. For example, even StackOverflow has banned any AI-generated content over its community~\cite{}, the erosion of community value still happened as discussed above. A study also found unintended consequence of banning AI-generated content in StackExchange: increase of human-made low-quality answers and a potential decline in new questions~\cite{wang2024can}. %\mc{love the examples from Stack overflow etc}

\mc{STOPPED HERE}
\subsection{Harms Produced without Intentions}
In online platform content moderation, problematic content is often assumed to be produced by malicious actors. Even in misinformation contexts where distributors may not be intentionally harmful, the dominant threat model still foregrounds an adversary who seeks to deceive---called misinformation or disinformation campaign. AI-generated content shares some similarity with misinformation in this regard, yet it further expands the space of unintentional harms: creators can adopt generative AI for routine content creation, where the harms like content authenticity arise.

To some extent, the trade-off between entertainment and harm has existed prior to the recent surge of generative AI products. For instance, satirical or playful manipulations have long circulated online~\cite{}. However, before large generative models became widely accessible, much of the entertainment-oriented manipulated media remained ``cheapfakes'' that viewers could more readily recognize. By contrast, today’s generative AI tools substantially lower the threshold for producing realistic media, making inadvertent deception and misleading more likely even in everyday creative workflows~\cite{}.

These unintentional harms are further amplified by the weak guidance on responsible AI use in content creation. Previous work on short video content creators found their urgent needs for platforms to provide tools and guidelines for creating `ethical and responsible’ content, regarding the unintentional rule-breaking the responsibility as creators to audiences~\cite{kim2024unlocking}. However, study also reveal that to date, only a few platforms have offered concrete guideline on how generative AI should be responsibly used in content creation beyond the guidance on AI-generated content disclosure~\cite{gao2026governance}.




% Note: basically two problems: 1. harm is dynamical, 2. the boundary of issues are vague




